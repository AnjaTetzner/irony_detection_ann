{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\twitter\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "output= 'output_trainTestData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "def text_to_wordlist(tweet, vocab):\n",
    "    #Remove hashtags\n",
    "    #tweet = re.sub('#','', tweet)\n",
    "    \n",
    "    #Remove usermentions\n",
    "    tweet= re.sub(r'(\\w+|^|)@\\w+','', tweet)\n",
    "    \n",
    "    #Treats url's as special tokens (actually twitter specific)\n",
    "    tweet=re.sub(r'((http|https)://)(\\w|[.]|/)+', 'URL', tweet)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    vocab.update(tweet)\n",
    "    return tweet\n",
    "\n",
    "def process_tweets(list_sentences, vocab):\n",
    "    tweets = []\n",
    "    for tweet in list_sentences:\n",
    "        twt = text_to_wordlist(tweet, vocab)\n",
    "        tweets.append(twt)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train data, test data and word_index\n",
    "def create_train_test_wordindex(vocab, tweets, corpustrain):\n",
    "    MAX_NB_WORDS = len(vocab)\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH= 0\n",
    "    for tweet in tweets:\n",
    "        if len(tweet) > MAX_SEQUENCE_LENGTH:\n",
    "            MAX_SEQUENCE_LENGTH = len(tweet)\n",
    "            \n",
    "    print('Maximal Sequence Length: '+str(MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n",
    "\n",
    "    sequences = [[word_index.get(t, 0) for t in tweet]\n",
    "                 for tweet in tweets[:len(corpustrain)]]\n",
    "\n",
    "    test_sequences = [[word_index.get(t, 0) for t in tweet] \n",
    "                      for tweet in tweets[len(corpustrain):]]\n",
    "\n",
    "    train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                         padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "    test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n",
    "                              truncating=\"post\")\n",
    "\n",
    "    return train_data, test_data, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store data, labels and word_index for reproducability\n",
    "def write_file(filename, data):\n",
    "    file_present = glob.glob(filename)\n",
    "    if not file_present:\n",
    "        pd.DataFrame(data).to_csv(filename, header=None, index=None)\n",
    "    else:\n",
    "        print('Warining: File '+str(filename)+' already exists.')\n",
    "        \n",
    "def write_index(filename, index):\n",
    "    file_present = glob.glob(filename)\n",
    "    if not file_present:\n",
    "        f = open(filename, 'w+', encoding=\"utf-8\")\n",
    "        isnew = True\n",
    "        for key, value in index.items():\n",
    "            if isnew:\n",
    "                f.write(str(key)+'\\t'+str(value))\n",
    "                isnew = False\n",
    "            else:\n",
    "                f.write('\\n'+str(key)+'\\t'+str(value))\n",
    "        f.close()\n",
    "    else:\n",
    "        print('Warining: File '+str(filename)+' already exists.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SamEval2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_samEval(FILENAME):\n",
    "    labels = []\n",
    "    corpus = []   \n",
    "    with open(FILENAME, 'r', encoding='utf-8') as train: #Decoding utf-8, else exception\n",
    "        for line in train:\n",
    "            if not line.lower().startswith(\"tweet index\"): #skip header\n",
    "                line = line.rstrip().split(\"\\t\")\n",
    "                label = line[1] #erste Spalte - label\n",
    "                labels.append(int(label))\n",
    "                tweet = line[2] #zweite Spalte - tweet\n",
    "                corpus.append(tweet)  \n",
    "    returnvalue = []\n",
    "    returnvalue.append(labels)\n",
    "    returnvalue.append(corpus)\n",
    "    return returnvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMEVAL = path + 'SemEval2018-T3-train-taskA_emoji.txt'\n",
    "GOLD_TEST_SAMEVAL  = path + 'SemEval2018-T3_gold_test_taskA_emoji.txt'\n",
    "\n",
    "file_sameval_train_data = output + 'SamEval_train_data.csv'\n",
    "file_sameval_train_labels = output + 'SamEval_train_labels.csv'\n",
    "file_sameval_test_data = output + 'SamEval_test_data.csv'\n",
    "file_sameval_test_labels = output + 'SamEval_test_labels.csv'\n",
    "file_sameval_word_index = output + 'SamEval_word_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulttrain = read_data_samEval(TRAIN_SAMEVAL)\n",
    "labelstrain_sameval = resulttrain[0]\n",
    "corpustrain_sameval = resulttrain[1]\n",
    "\n",
    "resulttest = read_data_samEval(GOLD_TEST_SAMEVAL)\n",
    "labelstest_sameval = resulttest[0]\n",
    "corpustest_sameval = resulttest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train corpus: 3834\n",
      "Lenght train labels: 3834\n",
      "Lenght test corpus: 784\n",
      "lenght test labels: 784\n"
     ]
    }
   ],
   "source": [
    "print('Length train corpus: '+str(len(corpustrain_sameval)))\n",
    "print('Lenght train labels: '+str(len(labelstrain_sameval)))\n",
    "print('Lenght test corpus: '+str(len(corpustest_sameval)))\n",
    "print('lenght test labels: '+str(len(labelstest_sameval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number labels train data: Counter({0: 1923, 1: 1911})\n",
      "Number labels test data: Counter({0: 473, 1: 311})\n"
     ]
    }
   ],
   "source": [
    "print('Number labels train data: '+str(Counter(labelstrain_sameval)))\n",
    "print('Number labels test data: '+str(Counter(labelstest_sameval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "vocab_sameval = Counter()\n",
    "tweets_sameval = process_tweets(corpustrain_sameval + corpustest_sameval, vocab_sameval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 40\n"
     ]
    }
   ],
   "source": [
    "train_data_sameval, test_data_sameval, word_index_sameval = create_train_test_wordindex(vocab_sameval, tweets_sameval, corpustrain_sameval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(file_sameval_train_data, train_data_sameval)\n",
    "write_file(file_sameval_train_labels, labelstrain_sameval)\n",
    "write_file(file_sameval_test_data, test_data_sameval)\n",
    "write_file(file_sameval_test_labels, labelstest_sameval)\n",
    "write_index(file_sameval_word_index, word_index_sameval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMEVAL_B = path + 'SemEval2018-T3-train-taskB_emoji_ironyHashtags.txt'\n",
    "GOLD_TEST_SAMEVAL_B = path + 'SemEval2018-T3_gold_test_taskB_emoji.txt'\n",
    "\n",
    "file_sameval_b_train_data = output + 'SamEval_B_train_data.csv'\n",
    "file_sameval_b_train_labels = output + 'SamEval_B_train_labels.csv'\n",
    "file_sameval_b_test_data = output + 'SamEval_B_test_data.csv'\n",
    "file_sameval_b_test_labels = output + 'SamEval_B_test_labels.csv'\n",
    "file_sameval_b_word_index = output + 'SamEval_B_word_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulttrain_b = read_data_samEval(TRAIN_SAMEVAL_B)\n",
    "labelstrain_sameval_b = resulttrain_b[0]\n",
    "corpustrain_sameval_b = resulttrain_b[1]\n",
    "\n",
    "resulttest_b = read_data_samEval(GOLD_TEST_SAMEVAL_B)\n",
    "labelstest_sameval_b = resulttest_b[0]\n",
    "corpustest_sameval_b = resulttest_b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data overall: 4618\n",
      "Length train corpus: 3834\n",
      "Lenght train labels: 3834\n",
      "Lenght test corpus: 784\n",
      "lenght test labels: 784\n"
     ]
    }
   ],
   "source": [
    "print('Data overall: '+str(len(corpustrain_sameval_b+corpustest_sameval_b)))\n",
    "print('Length train corpus: '+str(len(corpustrain_sameval_b)))\n",
    "print('Lenght train labels: '+str(len(labelstrain_sameval_b)))\n",
    "print('Lenght test corpus: '+str(len(corpustest_sameval_b)))\n",
    "print('lenght test labels: '+str(len(labelstest_sameval_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number labels train data: Counter({0: 1923, 1: 1390, 2: 316, 3: 205})\n",
      "Number labels test data: Counter({0: 473, 1: 164, 2: 85, 3: 62})\n"
     ]
    }
   ],
   "source": [
    "print('Number labels train data: '+str(Counter(labelstrain_sameval_b)))\n",
    "print('Number labels test data: '+str(Counter(labelstest_sameval_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "vocab_sameval_b = Counter()\n",
    "tweets_sameval_b = process_tweets(corpustrain_sameval_b + corpustest_sameval_b, vocab_sameval_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 40\n"
     ]
    }
   ],
   "source": [
    "train_data_sameval_b, test_data_sameval_b, word_index_sameval_b = create_train_test_wordindex(vocab_sameval_b, tweets_sameval_b, corpustrain_sameval_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(file_sameval_b_train_data, train_data_sameval_b)\n",
    "write_file(file_sameval_b_train_labels, labelstrain_sameval_b)\n",
    "write_file(file_sameval_b_test_data, test_data_sameval_b)\n",
    "write_file(file_sameval_b_test_labels, labelstest_sameval_b)\n",
    "write_index(file_sameval_b_word_index, word_index_sameval_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with #irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMEVAL = path + 'SemEval2018-T3-train-taskA_emoji_ironyHashtags.txt'\n",
    "GOLD_TEST_SAMEVAL  = path + 'SemEval2018-T3_gold_test_taskA_emoji.txt'\n",
    "\n",
    "file_sameval_train_data = output + 'SamEval_train_data_hash.csv'\n",
    "file_sameval_train_labels = output + 'SamEval_train_labels_hash.csv'\n",
    "file_sameval_test_data = output + 'SamEval_test_data_hash.csv'\n",
    "file_sameval_test_labels = output + 'SamEval_test_labels_hash.csv'\n",
    "file_sameval_word_index = output + 'SamEval_word_index_hash.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulttrain = read_data_samEval(TRAIN_SAMEVAL)\n",
    "labelstrain_sameval = resulttrain[0]\n",
    "corpustrain_sameval = resulttrain[1]\n",
    "\n",
    "resulttest = read_data_samEval(GOLD_TEST_SAMEVAL)\n",
    "labelstest_sameval = resulttest[0]\n",
    "corpustest_sameval = resulttest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train corpus: 3834\n",
      "Lenght train labels: 3834\n",
      "Lenght test corpus: 784\n",
      "lenght test labels: 784\n"
     ]
    }
   ],
   "source": [
    "print('Length train corpus: '+str(len(corpustrain_sameval)))\n",
    "print('Lenght train labels: '+str(len(labelstrain_sameval)))\n",
    "print('Lenght test corpus: '+str(len(corpustest_sameval)))\n",
    "print('lenght test labels: '+str(len(labelstest_sameval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number labels train data: Counter({0: 1923, 1: 1911})\n",
      "Number labels test data: Counter({0: 473, 1: 311})\n"
     ]
    }
   ],
   "source": [
    "print('Number labels train data: '+str(Counter(labelstrain_sameval)))\n",
    "print('Number labels test data: '+str(Counter(labelstest_sameval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "vocab_sameval = Counter()\n",
    "tweets_sameval = process_tweets(corpustrain_sameval + corpustest_sameval, vocab_sameval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 40\n"
     ]
    }
   ],
   "source": [
    "train_data_sameval, test_data_sameval, word_index_sameval = create_train_test_wordindex(vocab_sameval, tweets_sameval, corpustrain_sameval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(file_sameval_train_data, train_data_sameval)\n",
    "write_file(file_sameval_train_labels, labelstrain_sameval)\n",
    "write_file(file_sameval_test_data, test_data_sameval)\n",
    "write_file(file_sameval_test_labels, labelstest_sameval)\n",
    "write_index(file_sameval_word_index, word_index_sameval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reyes, Rosso and Veale (2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRONY = path + 'Irony.txt'\n",
    "EDUCATION = path + 'Education.txt'\n",
    "HUMOR = path + 'Humor.txt'\n",
    "POLITICS = path + 'Politics.txt'\n",
    "\n",
    "file_reyesall_train_data = output + 'reyes_train_data.csv'\n",
    "file_reyesall_train_labelsbinary = output + 'reyes_train_labelsbinary.csv'\n",
    "file_reyesall_train_labelsmulty = output + 'reyes_train_labelsmulty.csv'\n",
    "file_reyesall_test_data = output + 'reyes_test_data.csv'\n",
    "file_reyesall_test_labelsbinary = output + 'reyes_test_labelsbinary.csv'\n",
    "file_reyesall_test_labelsmulty = output + 'reyes_test_labelmulty.csv'\n",
    "file_reyesall_word_index = output + 'reyes_word_index.csv'\n",
    "\n",
    "file_reyesivse_train_data = output + 'reyes_i_e_train_data.csv'\n",
    "file_reyesivse_train_labels = output + 'reyes_i_e_train_labels.csv'\n",
    "file_reyesivse_test_data = output + 'reyes_i_e_test_data.csv'\n",
    "file_reyesivse_test_labels = output + 'reyes_i_e_test_labels.csv'\n",
    "file_reyesivse_word_index = output + 'reyes_i_e_word_index.csv'\n",
    "\n",
    "file_reyesivsh_train_data = output + 'reyes_i_h_train_data.csv'\n",
    "file_reyesivsh_train_labels = output + 'reyes_i_h_train_labels.csv'\n",
    "file_reyesivsh_test_data = output + 'reyes_i_h_test_data.csv'\n",
    "file_reyesivsh_test_labels = output + 'reyes_i_h_test_labels.csv'\n",
    "file_reyesivsh_word_index = output + 'reyes_i_h_word_index.csv'\n",
    "\n",
    "file_reyesivsp_train_data = output + 'reyes_i_p_train_data.csv'\n",
    "file_reyesivsp_train_labels = output + 'reyes_i_p_train_labels.csv'\n",
    "file_reyesivsp_test_data = output + 'reyes_i_p_test_data.csv'\n",
    "file_reyesivsp_test_labels = output + 'reyes_i_p_test_labels.csv'\n",
    "file_reyesivsp_word_index = output + 'reyes_i_p_word_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data per file\n",
      "Number data Irony: 9964\n",
      "Number data Education: 9832\n",
      "Number data Humor: 9938\n",
      "Number data Politics: 8461\n",
      "-------------------------\n",
      "Number data altogether: 38195\n",
      "Number data Irony vs. Education: 19796\n",
      "Number data Irony vs Humor: 19902\n",
      "Number data Irony vs. Politics: 18425\n",
      "------------------------------\n",
      "Number train data overall: 30556\n",
      "Number test data overall: 7639\n",
      "-------------------------\n",
      "Number train data Irony vs Education: 15836\n",
      "Number test data Irony vs. Education: 3960\n",
      "-------------------------\n",
      "Number train data Irony vs. Humor: 15921\n",
      "Number test data Irony vs. Humor: 3981\n",
      "-------------------------\n",
      "Number train data Irony vs. Politics: 14740\n",
      "Number test data Irony vs. Politics: 3685\n"
     ]
    }
   ],
   "source": [
    "#Read the data from file\n",
    "def read_data(FILENAME, labelbinary, labelmulty):\n",
    "    result = []\n",
    "    with open(FILENAME, 'r', encoding='utf-8') as train:\n",
    "        for line in train:\n",
    "            listentry = []\n",
    "            line = line.rstrip().split(\"\\n\")\n",
    "            listentry.append(line)\n",
    "            listentry.append(labelbinary)\n",
    "            listentry.append(labelmulty)\n",
    "            result.append(listentry)\n",
    "    return result\n",
    "\n",
    "#define Labels\n",
    "IRONYLABEL = 1\n",
    "NONIRONYLABEL = 0\n",
    "MULTY = 0\n",
    "\n",
    "ironycorpus = read_data(IRONY, IRONYLABEL, MULTY)\n",
    "educationcorpus = read_data(EDUCATION, NONIRONYLABEL, MULTY+1)\n",
    "humorcorpus = read_data(HUMOR, NONIRONYLABEL, MULTY+2)\n",
    "politicscorpus = read_data(POLITICS, NONIRONYLABEL, MULTY+3)\n",
    "\n",
    "print(\"Number data per file\")\n",
    "print(\"Number data Irony: \" + str(len(ironycorpus)))\n",
    "print(\"Number data Education: \" + str(len(educationcorpus)))\n",
    "print(\"Number data Humor: \" + str(len(humorcorpus)))\n",
    "print(\"Number data Politics: \" + str(len(politicscorpus)))\n",
    "print(\"-------------------------\")\n",
    "\n",
    "#Concatenate the individual corpuses to create a single data set and vs-datasets\n",
    "corpus = np.concatenate((ironycorpus, educationcorpus, humorcorpus, politicscorpus))\n",
    "ironyvseducation = np.concatenate((ironycorpus, educationcorpus))\n",
    "ironyvshumor = np.concatenate((ironycorpus, humorcorpus))\n",
    "ironyvspolitics = np.concatenate ((ironycorpus, politicscorpus))\n",
    "\n",
    "print(\"Number data altogether: \" + str(len(corpus)))\n",
    "print(\"Number data Irony vs. Education: \" + str(len(ironyvseducation)))\n",
    "print(\"Number data Irony vs Humor: \" + str(len(ironyvshumor)))\n",
    "print(\"Number data Irony vs. Politics: \" + str(len(ironyvspolitics)))\n",
    "print(\"------------------------------\")\n",
    "\n",
    "#Create random train and test data sets\n",
    "\n",
    "#Randomize the data\n",
    "corpus = random.sample(list(corpus),len(corpus))\n",
    "ironyvseducation = random.sample(list(ironyvseducation), len(ironyvseducation))\n",
    "ironyvshumor = random.sample(list(ironyvshumor), len(ironyvshumor))\n",
    "ironyvspolitics = random.sample(list(ironyvspolitics), len(ironyvspolitics))\n",
    "\n",
    "numbertraindata = int((len(corpus))*0.8)\n",
    "numbertrainironyvseducation = int((len(ironyvseducation))*0.8)\n",
    "numbertrainironyvshumor = int((len(ironyvshumor))*0.8)\n",
    "numbertrainironyvspolitics = int((len(ironyvspolitics))*0.8)\n",
    "\n",
    "train = corpus[:numbertraindata]\n",
    "test = corpus[numbertraindata:]\n",
    "\n",
    "trainironyvseducation = ironyvseducation[:numbertrainironyvseducation]\n",
    "testironyvseducation = ironyvseducation[numbertrainironyvseducation:]\n",
    "\n",
    "trainironyvshumor = ironyvshumor[:numbertrainironyvshumor]\n",
    "testironyvshumor = ironyvshumor[numbertrainironyvshumor:]\n",
    "\n",
    "trainironyvspolitics = ironyvspolitics[:numbertrainironyvspolitics]\n",
    "testironyvspolitics = ironyvspolitics[numbertrainironyvspolitics:]\n",
    "\n",
    "print(\"Number train data overall: \" + str(len(train)))\n",
    "print(\"Number test data overall: \" + str(len(test)))\n",
    "print(\"-------------------------\")\n",
    "print(\"Number train data Irony vs Education: \" + str(len(trainironyvseducation)))\n",
    "print(\"Number test data Irony vs. Education: \" + str(len(testironyvseducation)))\n",
    "print(\"-------------------------\")\n",
    "print(\"Number train data Irony vs. Humor: \" + str(len(trainironyvshumor)))\n",
    "print(\"Number test data Irony vs. Humor: \" + str(len(testironyvshumor)))\n",
    "print(\"-------------------------\")\n",
    "print(\"Number train data Irony vs. Politics: \" + str(len(trainironyvspolitics)))\n",
    "print(\"Number test data Irony vs. Politics: \" + str(len(testironyvspolitics)))\n",
    "\n",
    "\n",
    "#Split the data into text and labels\n",
    "def split_data(corpus):\n",
    "    data = []\n",
    "    labelsbinary =[]\n",
    "    labelsmulty =[]\n",
    "    \n",
    "    for entry in corpus:\n",
    "        text = entry[0] #entry[0] is a list by istself but, what is needed is the string text\n",
    "        data.append(text[0])\n",
    "        labelsbinary.append(entry[1])\n",
    "        labelsmulty.append(entry[2])\n",
    "        \n",
    "    return data, labelsbinary, labelsmulty\n",
    "\n",
    "corpustrain_reyes, labelstrainbinary, labelstrainmulty = split_data(train)\n",
    "corpustest_reyes, labelstestbinary, labelstestmulty = split_data(test)  \n",
    "\n",
    "ironyvseducation_train, ironyvseducation_labelstrainbinary, ironyvseducation_labelstrainmulty = split_data(trainironyvseducation)\n",
    "ironyvseducation_test, ironyvseducation_labelstestbinary, ironyvseducation_labelstestmulty = split_data(testironyvseducation) \n",
    "\n",
    "ironyvshumor_train, ironyvshumor_labelstrainbinary, ironyvshumor_labelstrainmulty = split_data(trainironyvshumor)\n",
    "ironyvshumor_test, ironyvshumor_labelstestbinary, ironyvshumor_labelstestmulty = split_data(testironyvshumor) \n",
    "\n",
    "ironyvspolitics_train, ironyvspolitics_labelstrainbinary, ironyvspolitics_labelstrainmulty = split_data(trainironyvspolitics)\n",
    "ironyvspolitics_test, ironyvspolitics_labelstestbinary, ironyvspolitics_labelstestmulty = split_data(testironyvspolitics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "\n",
    "#all data\n",
    "vocab_reyes_all = Counter()\n",
    "tweets_reyes = process_tweets(corpustrain_reyes + corpustest_reyes, vocab_reyes_all)\n",
    "\n",
    "#Irony vs Education\n",
    "vocab_ironyvseducation = Counter()\n",
    "tweetsironyvseducation = process_tweets(ironyvseducation_train + ironyvseducation_test, vocab_ironyvseducation)\n",
    "\n",
    "#Irony vs Humor\n",
    "vocab_ironyvshumor = Counter()\n",
    "tweetsironyvshumor = process_tweets(ironyvshumor_train + ironyvshumor_test, vocab_ironyvshumor)\n",
    "\n",
    "#Irony vs Politics\n",
    "vocab_ironyvspolitics = Counter()\n",
    "tweetsironyvspolitics = process_tweets(ironyvspolitics_train + ironyvspolitics_test, vocab_ironyvspolitics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 167\n",
      "Maximal Sequence Length: 99\n",
      "Maximal Sequence Length: 90\n",
      "Maximal Sequence Length: 167\n"
     ]
    }
   ],
   "source": [
    "#Create train data, test data and word index for Reyes, Rosso and Veale\n",
    "\n",
    "#all Data\n",
    "train_data_reyes, test_data_reyes, word_index_reyes = create_train_test_wordindex(vocab_reyes_all, tweets_reyes, corpustrain_reyes)\n",
    "\n",
    "#Irony vs education\n",
    "ironyvseducation_train_data, ironyvseducation_test_data, ironyvseducation_word_index = create_train_test_wordindex(vocab_ironyvseducation, tweetsironyvseducation, ironyvseducation_train)\n",
    "\n",
    "#Irony vs Humor\n",
    "ironyvshumor_train_data, ironyvshumor_test_data, ironyvshumor_word_index = create_train_test_wordindex(vocab_ironyvshumor, tweetsironyvshumor, ironyvshumor_train)\n",
    "\n",
    "#Irony vs Politics\n",
    "ironyvspolitics_train_data, ironyvspolitics_test_data, ironyvspolitics_word_index = create_train_test_wordindex(vocab_ironyvspolitics, tweetsironyvspolitics, ironyvspolitics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store train data, test data and word index fpr Reyes, Rosso and Veale for reproducability\n",
    "\n",
    "#all data\n",
    "write_file(file_reyesall_train_data, train_data_reyes)\n",
    "write_file(file_reyesall_train_labelsbinary, labelstrainbinary)\n",
    "write_file(file_reyesall_train_labelsmulty, labelstrainmulty)\n",
    "write_file(file_reyesall_test_data, test_data_reyes)\n",
    "write_file(file_reyesall_test_labelsbinary, labelstestbinary)\n",
    "write_file(file_reyesall_test_labelsmulty, labelstestmulty)\n",
    "write_index(file_reyesall_word_index, word_index_reyes)\n",
    "\n",
    "#Irony vs Education\n",
    "write_file(file_reyesivse_train_data, ironyvseducation_train_data)\n",
    "write_file(file_reyesivse_train_labels, ironyvseducation_labelstrainbinary)\n",
    "write_file(file_reyesivse_test_data, ironyvseducation_test_data)\n",
    "write_file(file_reyesivse_test_labels, ironyvseducation_labelstestbinary)\n",
    "write_index(file_reyesivse_word_index, ironyvseducation_word_index)\n",
    "\n",
    "#Irony vs Humor\n",
    "write_file(file_reyesivsh_train_data, ironyvshumor_train_data)\n",
    "write_file(file_reyesivsh_train_labels, ironyvshumor_labelstrainbinary)\n",
    "write_file(file_reyesivsh_test_data, ironyvshumor_test_data)\n",
    "write_file(file_reyesivsh_test_labels, ironyvshumor_labelstestbinary)\n",
    "write_index(file_reyesivsh_word_index, ironyvshumor_word_index)\n",
    "\n",
    "#Irony vs Politics\n",
    "write_file(file_reyesivsp_train_data, ironyvspolitics_train_data)\n",
    "write_file(file_reyesivsp_train_labels, ironyvspolitics_labelstrainbinary)\n",
    "write_file(file_reyesivsp_test_data, ironyvspolitics_test_data)\n",
    "write_file(file_reyesivsp_test_labels, ironyvspolitics_labelstestbinary)\n",
    "write_index(file_reyesivsp_word_index, ironyvspolitics_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ghosh, Fabri, Muresan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ghosh_train = path + 'Ghosh_sarcasm_v2_tab_train.txt'\n",
    "Ghosh_test  = path + 'Ghosh_sarcasm_v2_tab_test.txt'\n",
    "Ghosh_validate  = path + 'Ghosh_sarcasm_v2_tab_valid.txt'\n",
    "\n",
    "#because of cross-valdiation train and valid data are concatinated\n",
    "file_ghosh_train_data = output + 'Ghosh_train_data.csv'b\n",
    "file_ghosh_train_labels = output + 'Ghosh_train_labels.csv'\n",
    "file_ghosh_test_data = output + 'Ghosh_test_data.csv'\n",
    "file_ghosh_test_labels = output + 'Ghosh_test_labels.csv'\n",
    "file_ghosh_word_index = output + 'Ghosh_word_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train data overall: 4224\n",
      "Number test data overall: 468\n"
     ]
    }
   ],
   "source": [
    "def read_data_ghosh(FILENAME):\n",
    "    corpus = []   \n",
    "    with open(FILENAME, 'r', encoding='utf-8') as train: #Decoding utf-8, else exception\n",
    "        for line in train:\n",
    "            listentry = []\n",
    "            label = int(line.rstrip().split()[0].rstrip().split('.')[0]) # label\n",
    "            tweet = line.lstrip()[4:].rstrip().split('/n') #tweet\n",
    "            listentry.append(tweet)\n",
    "            listentry.append(label)\n",
    "            corpus.append(listentry)\n",
    "    return corpus\n",
    "\n",
    "resulttrain_ghosh = read_data_ghosh(Ghosh_train)\n",
    "resultvalid_ghosh = read_data_ghosh(Ghosh_validate)\n",
    "#concatinate\n",
    "resulttrain_all_ghosh = np.concatenate((resulttrain_ghosh,resultvalid_ghosh))\n",
    "#randomize train\n",
    "resulttrain_all_ghosh = random.sample(list(resulttrain_all_ghosh), len(resulttrain_all_ghosh))\n",
    "\n",
    "resulttest_ghosh = read_data_ghosh(Ghosh_test)\n",
    "#randomize test\n",
    "resulttest_ghosh = random.sample(list(resulttest_ghosh), len(resulttest_ghosh))\n",
    "\n",
    "def split_data(corpus):\n",
    "    data = []\n",
    "    labelsbinary =[]\n",
    "    \n",
    "    for entry in corpus:\n",
    "        text = entry[0] #entry[0] is a list by istself but, what is needed is the string text\n",
    "        data.append(text[0])\n",
    "        labelsbinary.append(entry[1])\n",
    "        \n",
    "    return data, labelsbinary\n",
    "\n",
    "train_ghosh, trainlabels_ghosh = split_data(resulttrain_all_ghosh)\n",
    "test_ghosh, testlabels_ghosh = split_data(resulttest_ghosh)\n",
    "\n",
    "print(\"Number train data overall: \" + str(len(train_ghosh)))\n",
    "print(\"Number test data overall: \" + str(len(resulttest_ghosh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number labels train data: Counter({0: 2112, 1: 2112})\n",
      "Number labels test data: Counter({1: 234, 0: 234})\n"
     ]
    }
   ],
   "source": [
    "print('Number labels train data: '+str(Counter(trainlabels_ghosh)))\n",
    "print('Number labels test data: '+str(Counter(testlabels_ghosh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "vocab_ghosh = Counter()\n",
    "tweets_ghosh = process_tweets(train_ghosh+test_ghosh, vocab_ghosh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 1894\n"
     ]
    }
   ],
   "source": [
    "#Create train data, test data and word index for Reyes, Rosso and Veale\n",
    "train_data_ghosh, test_data_ghosh, word_index_ghosh = create_train_test_wordindex(vocab_ghosh, tweets_ghosh, train_ghosh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the train data, test data and word index for Ghosh, Fabri and Muresan\n",
    "#train\n",
    "write_file(file_ghosh_train_data, train_data_ghosh)\n",
    "write_file(file_ghosh_train_labels, train_labels_ghosh)\n",
    "#test\n",
    "write_file(file_ghosh_test_data, test_data_ghosh)\n",
    "write_file(file_ghosh_test_labels, test_labels_ghosh)\n",
    "#index\n",
    "write_index(file_ghosh_word_index, word_index_ghosh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IronITA 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IRONITA = path + 'training_ironita2018.csv'\n",
    "TEST_IRONITA  = path + 'test_gold_ironita2018.csv'\n",
    "\n",
    "file_ironita_train_data = output + 'ironita_train_data.csv'\n",
    "file_ironita_train_labels = output + 'ironita_train_labels.csv'\n",
    "file_ironita_train_labels_sarcasm = output + 'ironita_train_labels_sarcasm.csv'\n",
    "file_ironita_test_data = output + 'ironita_test_data.csv'\n",
    "file_ironita_test_labels = output + 'ironita_test_labels.csv'\n",
    "file_ironita_test_labels_sarcasm = output + 'ironita_test_labels_sarcasm.csv'\n",
    "file_ironita_word_index = output + 'ironita_word_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_ironita(FILENAME):\n",
    "    labels = []\n",
    "    labels_sarcasm = []\n",
    "    corpus = []   \n",
    "    with open(FILENAME, 'r', encoding='utf-8') as train: #Decoding utf-8, else exception\n",
    "        for line in train:\n",
    "            if not line.lower().startswith(\"id\ttext\tirony\tsarcasm\ttopic\"): #skip header\n",
    "                line = line.rstrip().split(\"\\t\")\n",
    "                label = line[2]\n",
    "                labels.append(int(label))\n",
    "                labels_sarcasm.append(line[3])\n",
    "                corpus.append(line[1])  \n",
    "    returnvalue = []\n",
    "    returnvalue.append(labels)\n",
    "    returnvalue.append(labels_sarcasm)\n",
    "    returnvalue.append(corpus)\n",
    "    return returnvalue\n",
    "\n",
    "resulttrain = read_data_ironita(TRAIN_IRONITA)\n",
    "labelstrain_ironita = resulttrain[0]\n",
    "labelstrain_sarcasm_ironita = resulttrain[1]\n",
    "corpustrain_ironita = resulttrain[2]\n",
    "\n",
    "resulttest = read_data_ironita(TEST_IRONITA)\n",
    "labelstest_ironita = resulttest[0]\n",
    "labelstest_sarcasm_ironita = resulttest[1]\n",
    "corpustest_ironita = resulttest[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train corpus: 3977\n",
      "Lenght train labels: 3977\n",
      "Lenght test corpus: 872\n",
      "lenght test labels: 872\n"
     ]
    }
   ],
   "source": [
    "print('Length train corpus: '+str(len(corpustrain_ironita)))\n",
    "print('Lenght train labels: '+str(len(labelstrain_ironita)))\n",
    "print('Lenght test corpus: '+str(len(corpustest_ironita)))\n",
    "print('lenght test labels: '+str(len(labelstest_ironita)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number labels train data: Counter({1: 2023, 0: 1954})\n",
      "Number labels test data: Counter({0: 437, 1: 435})\n"
     ]
    }
   ],
   "source": [
    "print('Number labels train data: '+str(Counter(labelstrain_ironita)))\n",
    "print('Number labels test data: '+str(Counter(labelstest_ironita)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ironitab = []\n",
    "leng = len(resulttrain[0])\n",
    "for i in range(leng):\n",
    "    if resulttrain[0][i] == 0 and resulttrain[1][i] == '0':\n",
    "        count_ironitab.append('not-ironic')\n",
    "    if resulttrain[0][i] == 1 and resulttrain[1][i] == '0':\n",
    "        count_ironitab.append('irony_not_sarcasm')\n",
    "    if resulttrain[0][i] == 1 and resulttrain[1][i] == '1':\n",
    "        count_ironitab.append('sarcasm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'not-ironic': 1954, 'irony_not_sarcasm': 1110, 'sarcasm': 913})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(count_ironitab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ironitab_test = []\n",
    "leng = len(resulttest[0])\n",
    "for i in range(leng):\n",
    "    if resulttest[0][i] == 0 and resulttest[1][i] == '0':\n",
    "        count_ironitab_test.append('not-ironic')\n",
    "    if resulttest[0][i] == 1 and resulttest[1][i] == '0':\n",
    "        count_ironitab_test.append('irony_not_sarcasm')\n",
    "    if resulttest[0][i] == 1 and resulttest[1][i] == '1':\n",
    "        count_ironitab_test.append('sarcasm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'irony_not_sarcasm': 219, 'sarcasm': 216, 'not-ironic': 437})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(count_ironitab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "vocab_ironita = Counter()\n",
    "tweets_ironita = process_tweets(corpustrain_ironita + corpustest_ironita, vocab_ironita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 39\n"
     ]
    }
   ],
   "source": [
    "train_data_ironita, test_data_ironita, word_index_ironita = create_train_test_wordindex(vocab_ironita, tweets_ironita, corpustrain_ironita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(file_ironita_train_data, train_data_ironita)\n",
    "write_file(file_ironita_train_labels, labelstrain_ironita)\n",
    "write_file(file_ironita_train_labels_sarcasm, labelstrain_sarcasm_ironita)\n",
    "write_file(file_ironita_test_data, test_data_ironita)\n",
    "write_file(file_ironita_test_labels, labelstest_ironita)\n",
    "write_file(file_ironita_test_labels_sarcasm, labelstest_sarcasm_ironita)\n",
    "write_index(file_ironita_word_index, word_index_ironita)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ravi and Ravie 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRONY = path + 'Ravi_finalpos499.txt'\n",
    "NON_IRONY = path + 'Ravi_finalNeg2498.csv'\n",
    "\n",
    "file_ravi_train_data = output + 'ravi_train_data.csv'\n",
    "file_ravi_train_labels = output + 'ravi_train_labels.csv'\n",
    "file_ravi_test_data = output + 'ravi_test_data.csv'\n",
    "file_ravi_test_labels = output + 'ravi_test_labelsb.csv'\n",
    "file_ravi_word_index = output + 'ravi_word_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train: 2397\n",
      "Number test: 600\n"
     ]
    }
   ],
   "source": [
    "def read_data_ravi(FILENAME, label):\n",
    "    result = []\n",
    "    with open(FILENAME, 'r', encoding='utf-8') as train:\n",
    "        for line in train:\n",
    "            listentry = []\n",
    "            line = line.rstrip().split(\"\\n\")\n",
    "            listentry.append(line)\n",
    "            listentry.append(label)\n",
    "            result.append(listentry)\n",
    "    return result\n",
    "\n",
    "#define Labels\n",
    "IRONYLABEL = 1\n",
    "NONIRONYLABEL= 0\n",
    "\n",
    "ironycorpus = read_data_ravi(IRONY, IRONYLABEL)\n",
    "nonironycorpus = read_data_ravi(NON_IRONY, NONIRONYLABEL)\n",
    "\n",
    "#Concatenate irony and non-irony\n",
    "corpus = np.concatenate((ironycorpus, nonironycorpus))\n",
    "\n",
    "#Randomize the data\n",
    "corpus = random.sample(list(corpus),len(corpus))\n",
    "\n",
    "#Split data in train and test data\n",
    "numbertraindata = int((len(corpus))*0.8)\n",
    "train = corpus[:numbertraindata]\n",
    "test = corpus[numbertraindata:]\n",
    "\n",
    "#Split the data into text and labels\n",
    "def split_data(corpus):\n",
    "    data = []\n",
    "    labels =[]\n",
    "    \n",
    "    for entry in corpus:\n",
    "        text = entry[0] #entry[0] is a list by istself but, what is needed is the string text\n",
    "        data.append(text[0])\n",
    "        labels.append(entry[1])\n",
    "        \n",
    "    return data, labels\n",
    "\n",
    "corpustrain_ravi, labelstrain= split_data(train)\n",
    "corpustest_ravi, labelstest= split_data(test)\n",
    "\n",
    "print(\"Number train: \" + str(len(train)))\n",
    "print(\"Number test: \" + str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number labels train data: Counter({0: 2003, 1: 394})\n",
      "Number labels test data: Counter({0: 495, 1: 105})\n"
     ]
    }
   ],
   "source": [
    "print('Number labels train data: '+str(Counter(labelstrain)))\n",
    "print('Number labels test data: '+str(Counter(labelstest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "vocab_ravi = Counter()\n",
    "tweets_ravi = process_tweets(corpustrain_ravi + corpustest_ravi, vocab_ravi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 1233\n"
     ]
    }
   ],
   "source": [
    "#Create train data, test data and word index for ravi, Rosso and Veale\n",
    "train_data_ravi, test_data_ravi, word_index_ravi = create_train_test_wordindex(vocab_ravi, tweets_ravi, corpustrain_ravi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store train data, test data and word index for Ravi and Ravi for reproducability\n",
    "write_file(file_ravi_train_data, train_data_ravi)\n",
    "write_file(file_ravi_train_labels, labelstrain)\n",
    "write_file(file_ravi_test_data, test_data_ravi)\n",
    "write_file(file_ravi_test_labels, labelstest)\n",
    "write_index(file_ravi_word_index, word_index_ravi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiPOLC 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SENTIPOLC = path + 'training_set_sentipolc16.csv'\n",
    "TEST_SENTIPOLC = path + 'test_set_sentipolc16_gold2000.csv'\n",
    "\n",
    "file_sentipolc_train_data = output + 'sentipolc_train_data.csv'\n",
    "file_sentipolc_train_label_irony = output + 'sentipolc_train_labelirony.csv'\n",
    "file_sentipolc_train_label_opos = output + 'sentipolc_train_labelopos.csv'\n",
    "file_sentipolc_train_label_oneg = output + 'sentipolc_train_labeloneg.csv'\n",
    "file_sentipolc_train_label_lpos = output + 'sentipolc_train_labellpos.csv'\n",
    "file_sentipolc_train_label_lneg = output + 'sentipolc_train_labellneg.csv'\n",
    "file_sentipolc_test_data = output + 'sentipolc_test_data.csv'\n",
    "file_sentipolc_test_label_irony = output + 'sentipolc_test_labelirony.csv'\n",
    "file_sentipolc_test_label_opos = output + 'sentipolc_test_labelopos.csv'\n",
    "file_sentipolc_test_label_oneg = output + 'sentipolc_test_labeloneg.csv'\n",
    "file_sentipolc_test_label_lpos = output + 'sentipolc_test_labellpos.csv'\n",
    "file_sentipolc_test_label_lneg = output + 'sentipolc_test_labelneg.csv'\n",
    "file_sentipolc_word_index = output + 'sentipolc_word_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\twitter\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Read data\n",
    "sentipolc_train = pd.read_csv(TRAIN_SENTIPOLC)\n",
    "sentipolc_test = pd.read_csv(TEST_SENTIPOLC, sep = '\",\"', header = None, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataframe in text and labels for training\n",
    "train_labelsirony = sentipolc_train['iro']\n",
    "train_labelsopos = sentipolc_train['opos']\n",
    "train_labelsoneg = sentipolc_train['oneg']\n",
    "train_labelslpos = sentipolc_train['lpos']\n",
    "train_labelslneg = sentipolc_train['lneg']\n",
    "train_corpus_sentipolc = sentipolc_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataframe in text and labels for testing\n",
    "test_labelsirony = sentipolc_test[4]\n",
    "test_labelsopos = sentipolc_test[2]\n",
    "test_labelsoneg = sentipolc_test[3]\n",
    "test_labelslpos = sentipolc_test[5]\n",
    "test_labelslneg = sentipolc_test[6]\n",
    "test_corpus_sentipolc = sentipolc_test[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train corpus: 7410\n",
      "Lenght train labels: 7410\n",
      "Lenght test corpus: 2000\n",
      "lenght test labels: 2000\n"
     ]
    }
   ],
   "source": [
    "print('Length train corpus: '+str(len(train_corpus_sentipolc)))\n",
    "print('Lenght train labels: '+str(len(train_labelsirony)))\n",
    "print('Lenght test corpus: '+str(len(test_corpus_sentipolc)))\n",
    "print('lenght test labels: '+str(len(test_labelsirony)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number labels train data: Counter({0: 6542, 1: 868})\n",
      "Number labels test data: Counter({0: 1765, 1: 235})\n"
     ]
    }
   ],
   "source": [
    "print('Number labels train data: '+str(Counter(train_labelsirony)))\n",
    "print('Number labels test data: '+str(Counter(test_labelsirony)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "traincorpus_sentipolc = train_corpus_sentipolc.tolist()\n",
    "testcorpus_sentipolc = test_corpus_sentipolc.tolist()\n",
    "\n",
    "vocab_sentipolc = Counter()\n",
    "tweets_sentipolc = process_tweets( traincorpus_sentipolc + testcorpus_sentipolc, vocab_sentipolc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 43\n"
     ]
    }
   ],
   "source": [
    "train_data_sentipolc, test_data_sentipolc, word_index_sentipolc = create_train_test_wordindex(vocab_sentipolc, tweets_sentipolc, traincorpus_sentipolc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the train data, test data and dictionary, before have to convert pandas series object to list objekt\n",
    "labelstrain_irony_sentipolc = train_labelsirony.tolist()\n",
    "labelstrain_opos_sentipolc = train_labelsopos.tolist()\n",
    "labelstrain_oneg_sentipolc = train_labelsoneg.tolist()\n",
    "labelstrain_lpos_sentipolc = train_labelslpos.tolist()\n",
    "labelstrain_lneg_sentipolc = train_labelslneg.tolist()\n",
    "\n",
    "labelstest_irony_sentipolc = test_labelsirony.tolist()\n",
    "labelstest_opos_sentipolc = test_labelsopos.tolist()\n",
    "labelstest_oneg_sentipolc = test_labelsoneg.tolist()\n",
    "labelstest_lpos_sentipolc = test_labelslpos.tolist()\n",
    "labelstest_lneg_sentipolc = test_labelslneg.tolist()\n",
    "\n",
    "write_file(file_sentipolc_train_data, train_data_sentipolc)\n",
    "write_file(file_sentipolc_train_label_irony, labelstrain_irony_sentipolc)\n",
    "write_file(file_sentipolc_train_label_opos, labelstrain_opos_sentipolc)\n",
    "write_file(file_sentipolc_train_label_oneg, labelstrain_oneg_sentipolc)\n",
    "write_file(file_sentipolc_train_label_lpos, labelstrain_lpos_sentipolc)\n",
    "write_file(file_sentipolc_train_label_lneg, labelstrain_lneg_sentipolc)\n",
    "\n",
    "write_file(file_sentipolc_test_data, test_data_sentipolc)\n",
    "write_file(file_sentipolc_test_label_irony, labelstest_irony_sentipolc)\n",
    "write_file(file_sentipolc_test_label_opos, labelstest_opos_sentipolc)\n",
    "write_file(file_sentipolc_test_label_oneg, labelstest_oneg_sentipolc)\n",
    "write_file(file_sentipolc_test_label_lpos, labelstest_lpos_sentipolc)\n",
    "write_file(file_sentipolc_test_label_lneg, labelstest_lneg_sentipolc)\n",
    "\n",
    "write_index(file_sentipolc_word_index, word_index_sentipolc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wallace, Cho, Kertz and Charnika (2014) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_WALLACE = path + 'Wallace_irony-labeled.csv'\n",
    "\n",
    "file_wallace_train_data = output + 'wallace_train_data.csv'\n",
    "file_wallace_train_labels = output + 'wallace_train_labels.csv'\n",
    "file_wallace_test_data = output + 'wallace_test_data.csv'\n",
    "file_wallace_test_labels = output + 'wallace_test_labels.csv'\n",
    "file_wallace_word_index = output + 'wallace_word_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wallace = pd.read_csv(DATA_WALLACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in train and test data\n",
    "numbertraindata = int((len(corpus_wallace))*0.8)\n",
    "\n",
    "train_wallace = corpus_wallace[:numbertraindata]\n",
    "test_wallace = corpus_wallace[numbertraindata:]\n",
    "\n",
    "train_corpus_wallace = train_wallace['comment_text'].tolist()\n",
    "train_labels_wallace = train_wallace['label'].tolist()\n",
    "test_corpus_wallace = test_wallace['comment_text'].tolist()\n",
    "test_labels_wallace = test_wallace['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number labels train data: Counter({-1: 1122, 1: 437})\n",
      "Number labels test data: Counter({-1: 290, 1: 100})\n"
     ]
    }
   ],
   "source": [
    "print('Number labels train data: '+str(Counter(train_labels_wallace)))\n",
    "print('Number labels test data: '+str(Counter(test_labels_wallace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preproces the data\n",
    "vocab_wallace = Counter()\n",
    "tweets_wallace = process_tweets(train_corpus_wallace + test_corpus_wallace, vocab_wallace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 891\n"
     ]
    }
   ],
   "source": [
    "#Creat train data, test data and word index for Wallace et al.\n",
    "train_data_wallace, test_data_wallace, word_index_wallace = create_train_test_wordindex(vocab_wallace, tweets_wallace, train_corpus_wallace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the train data, test data and word index for Wallace et al.\n",
    "#train\n",
    "write_file(file_wallace_train_data, train_data_wallace)\n",
    "write_file(file_wallace_train_labels, train_labels_wallace)\n",
    "#test\n",
    "write_file(file_wallace_test_data, test_data_wallace)\n",
    "write_file(file_wallace_test_labels, test_labels_wallace)\n",
    "#index\n",
    "write_index(file_wallace_word_index, word_index_wallace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own (German) data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TETZNER = path + 'GermanIronyCorpus_cleaned.txt'\n",
    "\n",
    "file_tetzner_train_data = output + 'tetzner_train_data.csv'\n",
    "file_tetzner_train_labels_polarity = output + 'tetzner_train_labels_polarity.csv'\n",
    "file_tetzner_train_labels_ironyform = output + 'tetzner_train_labels_ironyform.csv'\n",
    "file_tetzner_train_labels_change = output + 'tetzner_train_labels_change.csv'\n",
    "file_tetzner_train_labels_ironybinary = output + 'tetzner_train_labels_ironybinary.csv'\n",
    "file_tetzner_train_labels_ironymulty = output + 'tetzner_train_labels_ironymulty.csv'\n",
    "file_tetzner_test_data = output + 'tetzner_test_data.csv'\n",
    "file_tetzner_test_labels_polarity = output + 'tetzner_test_labels_polarity.csv'\n",
    "file_tetzner_test_labels_ironyform = output + 'tetzner_test_labels_ironyform.csv'\n",
    "file_tetzner_test_labels_change = output + 'tetzner_test_labels_change.csv'\n",
    "file_tetzner_test_labels_ironybinary = output + 'tetzner_test_labels_ironybinary.csv'\n",
    "file_tetzner_test_labels_ironymulty = output + 'tetzner_test_labels_ironymulty.csv'\n",
    "file_tetzner_word_index = output + 'tetzner_word_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "with open(DATA_TETZNER, 'r', encoding='utf-8-sig' ) as file: #Decoding utf-8, else exception\n",
    "    for line in file:\n",
    "        listentry = []\n",
    "        line = line.rstrip().split(\"\\t\")\n",
    "        listentry.append(line[0])# text\n",
    "        listentry.append(line[1])# polarity\n",
    "        listentry.append(line[2])# irony form\n",
    "        listentry.append(line[3])# change\n",
    "        listentry.append(line[4])# binary label\n",
    "        listentry.append(line[5])# multy label\n",
    "        result.append(listentry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test data\n",
    "result = random.sample(list(result),len(result))\n",
    "numbertraindata = int((len(result))*0.8)\n",
    "\n",
    "train_tetzner = result[:numbertraindata]\n",
    "test_tetzner = result[numbertraindata:]\n",
    "\n",
    "#Split the data into text and labels\n",
    "def split_data(corpus):\n",
    "    data = []\n",
    "    labels_polarity =[]\n",
    "    labels_ironyform =[]\n",
    "    labels_change = []\n",
    "    labels_ironybinary = []\n",
    "    labels_ironymulty = []\n",
    "    \n",
    "    for entry in corpus:\n",
    "        data.append(entry[0])\n",
    "        labels_polarity.append(entry[1])\n",
    "        labels_ironyform.append(entry[2])\n",
    "        labels_change.append(entry[3])\n",
    "        labels_ironybinary.append(entry[4])\n",
    "        labels_ironymulty.append(entry[5])\n",
    "\n",
    "    return data, labels_polarity, labels_ironyform, labels_change, labels_ironybinary, labels_ironymulty\n",
    "\n",
    "corpustrain_tetzner, labelstrainpolarity, labelstrainironyform, labelstrainchange, labelstrainironybinary, labelstrainironymulty = split_data(train_tetzner)\n",
    "corpustest_tetzner, labelstestpolarity, labelstestironyform, labelstestchange, labelstestironybinary, labelstestironymulty = split_data(test_tetzner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "vocab_tetzner = Counter()\n",
    "tweets_tetzner = process_tweets(corpustrain_tetzner + corpustest_tetzner, vocab_tetzner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 73\n"
     ]
    }
   ],
   "source": [
    "# Create train data, test data and dirctionary\n",
    "train_data_tetzner, test_data_tetzner, word_index_tetzner = create_train_test_wordindex(vocab_tetzner, tweets_tetzner, corpustrain_tetzner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the train data, test data and word index for the German Irony Corpus\n",
    "write_file(file_tetzner_train_data, train_data_tetzner)\n",
    "write_file(file_tetzner_train_labels_polarity, labelstrainpolarity)\n",
    "write_file(file_tetzner_train_labels_ironyform, labelstrainironyform)\n",
    "write_file(file_tetzner_train_labels_change, labelstrainchange)\n",
    "write_file(file_tetzner_train_labels_ironybinary, labelstrainironybinary)\n",
    "write_file(file_tetzner_train_labels_ironymulty, labelstrainironymulty)\n",
    "\n",
    "write_file(file_tetzner_test_data, test_data_tetzner)\n",
    "write_file(file_tetzner_test_labels_polarity, labelstestpolarity)\n",
    "write_file(file_tetzner_test_labels_ironyform, labelstestironyform)\n",
    "write_file(file_tetzner_test_labels_change, labelstestchange)\n",
    "write_file(file_tetzner_test_labels_ironybinary, labelstestironybinary)\n",
    "write_file(file_tetzner_test_labels_ironymulty, labelstestironymulty)\n",
    "\n",
    "write_index(file_tetzner_word_index, word_index_tetzner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own (German)  balanced data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TETZNER = path + 'GermanIronyCorpus_cleaned.txt'\n",
    "\n",
    "file_tetzner_train_data = output + 'tetzner_train_data_balanced.csv'\n",
    "file_tetzner_train_labels_polarity = output + 'tetzner_train_labels_polarity_balanced.csv'\n",
    "file_tetzner_train_labels_ironyform = output + 'tetzner_train_labels_ironyform_balanced.csv'\n",
    "file_tetzner_train_labels_change = output + 'tetzner_train_labels_change_balanced.csv'\n",
    "file_tetzner_train_labels_ironybinary = output + 'tetzner_train_labels_ironybinary_balanced.csv'\n",
    "file_tetzner_train_labels_ironymulty = output + 'tetzner_train_labels_ironymulty_balanced.csv'\n",
    "file_tetzner_test_data = output + 'tetzner_test_data_balanced.csv'\n",
    "file_tetzner_test_labels_polarity = output + 'tetzner_test_labels_polarity_balanced.csv'\n",
    "file_tetzner_test_labels_ironyform = output + 'tetzner_test_labels_ironyform_balanced.csv'\n",
    "file_tetzner_test_labels_change = output + 'tetzner_test_labels_change_balanced.csv'\n",
    "file_tetzner_test_labels_ironybinary = output + 'tetzner_test_labels_ironybinary_balanced.csv'\n",
    "file_tetzner_test_labels_ironymulty = output + 'tetzner_test_labels_ironymulty_balanced.csv'\n",
    "file_tetzner_word_index = output + 'tetzner_word_index_balanced.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "with open(DATA_TETZNER, 'r', encoding='utf-8-sig' ) as file: #Decoding utf-8, else exception\n",
    "    for line in file:\n",
    "        listentry = []\n",
    "        line = line.rstrip().split(\"\\t\")\n",
    "        listentry.append(line[0])# text\n",
    "        listentry.append(line[1])# polarity\n",
    "        listentry.append(line[2])# irony form\n",
    "        listentry.append(line[3])# change\n",
    "        listentry.append(line[4])# binary label\n",
    "        listentry.append(line[5])# multy label\n",
    "        result.append(listentry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4300"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test data\n",
    "result = random.sample(list(result),len(result))\n",
    "numbertraindata = int((len(result))*0.8)\n",
    "\n",
    "test_tetzner = result[numbertraindata:]\n",
    "train_tetzner = result[:numbertraindata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tetzner_irony = []\n",
    "train_tetzner_noirony = []\n",
    "for entry in train_tetzner:\n",
    "    if entry[4] == '1':\n",
    "        train_tetzner_irony.append(entry)\n",
    "    else:\n",
    "        train_tetzner_noirony.append(entry)\n",
    "count_irony_entries = len(train_tetzner_irony)\n",
    "train_tetzner = train_tetzner_irony + train_tetzner_noirony[:count_irony_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tetzner_irony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into text and labels\n",
    "def split_data(corpus):\n",
    "    data = []\n",
    "    labels_polarity =[]\n",
    "    labels_ironyform =[]\n",
    "    labels_change = []\n",
    "    labels_ironybinary = []\n",
    "    labels_ironymulty = []\n",
    "    \n",
    "    for entry in corpus:\n",
    "        data.append(entry[0])\n",
    "        labels_polarity.append(entry[1])\n",
    "        labels_ironyform.append(entry[2])\n",
    "        labels_change.append(entry[3])\n",
    "        labels_ironybinary.append(entry[4])\n",
    "        labels_ironymulty.append(entry[5])\n",
    "\n",
    "    return data, labels_polarity, labels_ironyform, labels_change, labels_ironybinary, labels_ironymulty\n",
    "\n",
    "corpustrain_tetzner, labelstrainpolarity, labelstrainironyform, labelstrainchange, labelstrainironybinary, labelstrainironymulty = split_data(train_tetzner)\n",
    "corpustest_tetzner, labelstestpolarity, labelstestironyform, labelstestchange, labelstestironybinary, labelstestironymulty = split_data(test_tetzner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1': 1087, '0': 1509})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labelstrainironybinary+labelstestironybinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "vocab_tetzner = Counter()\n",
    "tweets_tetzner = process_tweets(corpustrain_tetzner + corpustest_tetzner, vocab_tetzner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 69\n"
     ]
    }
   ],
   "source": [
    "# Create train data, test data and dirctionary\n",
    "train_data_tetzner, test_data_tetzner, word_index_tetzner = create_train_test_wordindex(vocab_tetzner, tweets_tetzner, corpustrain_tetzner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warining: File output_trainTestData/tetzner_train_data_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_train_labels_polarity_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_train_labels_ironyform_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_train_labels_change_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_train_labels_ironybinary_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_train_labels_ironymulty_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_test_data_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_test_labels_polarity_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_test_labels_ironyform_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_test_labels_change_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_test_labels_ironybinary_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_test_labels_ironymulty_balanced.csv already exists.\n",
      "Warining: File output_trainTestData/tetzner_word_index_balanced.csv already exists.\n"
     ]
    }
   ],
   "source": [
    "#Store the train data, test data and word index for the German Irony Corpus\n",
    "write_file(file_tetzner_train_data, train_data_tetzner)\n",
    "write_file(file_tetzner_train_labels_polarity, labelstrainpolarity)\n",
    "write_file(file_tetzner_train_labels_ironyform, labelstrainironyform)\n",
    "write_file(file_tetzner_train_labels_change, labelstrainchange)\n",
    "write_file(file_tetzner_train_labels_ironybinary, labelstrainironybinary)\n",
    "write_file(file_tetzner_train_labels_ironymulty, labelstrainironymulty)\n",
    "\n",
    "write_file(file_tetzner_test_data, test_data_tetzner)\n",
    "write_file(file_tetzner_test_labels_polarity, labelstestpolarity)\n",
    "write_file(file_tetzner_test_labels_ironyform, labelstestironyform)\n",
    "write_file(file_tetzner_test_labels_change, labelstestchange)\n",
    "write_file(file_tetzner_test_labels_ironybinary, labelstestironybinary)\n",
    "write_file(file_tetzner_test_labels_ironymulty, labelstestironymulty)\n",
    "\n",
    "write_index(file_tetzner_word_index, word_index_tetzner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_for_combination(filename, textposition, labelindicator, header, splitsign, labelexist):\n",
    "    result = []\n",
    "    with open(filename, 'r', encoding='utf-8') as train:\n",
    "        for line in train:\n",
    "            if not line.lower().startswith(header): #skip header, if exist\n",
    "                listentry = []\n",
    "                line = line.rstrip().split(splitsign)\n",
    "                if textposition != -1:\n",
    "                    listentry.append(line[textposition])\n",
    "                else:\n",
    "                    listentry.append(line)\n",
    "                if labelexist:\n",
    "                    listentry.append(int(line[labelindicator]))\n",
    "                else:\n",
    "                    listentry.append(int(labelindicator))\n",
    "                result.append(listentry)\n",
    "    return result\n",
    "\n",
    "def concatenate_train_test(corpuslist):\n",
    "    corpus = corpuslist[0]\n",
    "    j = len(corpuslist)\n",
    "    for i in range(1,j):\n",
    "        corpus = np.concatenate((corpus, corpuslist[i]))\n",
    "        \n",
    "    corpus = random.sample(list(corpus),len(corpus))\n",
    "    \n",
    "    numbertraindata = int((len(corpus))*0.8)\n",
    "    trainsplit = corpus[:numbertraindata]\n",
    "    testsplit = corpus[numbertraindata:]\n",
    "    \n",
    "    return trainsplit, testsplit\n",
    "\n",
    "def concatenate_train(corpuslist):\n",
    "    corpus = corpuslist[0]\n",
    "    j = len(corpuslist)\n",
    "    for i in range(1,j):\n",
    "        corpus = np.concatenate((corpus, corpuslist[i]))\n",
    "        \n",
    "    corpus = random.sample(list(corpus),len(corpus))\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "def split_text_label(corpus):\n",
    "    texts = []\n",
    "    labels =[]\n",
    "    \n",
    "    for entry in corpus:\n",
    "        text = entry[0] #entry[0] is a list by istself but, what is needed is the string text\n",
    "        texts.append(text)\n",
    "        labels.append(int(entry[1]))\n",
    "        \n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all english data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_ALL = path + 'english_datasets_all.csv'\n",
    "\n",
    "file_all_train_data = output + 'english_all_train_data_balanced.csv'\n",
    "file_all_train_labels = output +'english_all_train_labels_balanced.csv'\n",
    "file_all_test_data = output +'english_all_test_data_balanced.csv'\n",
    "file_all_test_labels = output +'english_all_test_labels_balanced.csv'\n",
    "file_all_word_index = output +'english_all_word_index_balanced.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_all = read_data_for_combination(ENGLISH_ALL,2 ,1 ,'filename\t' , '\\t', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test data\n",
    "result = random.sample(list(corpus_all),len(corpus_all))\n",
    "numbertraindata = int((len(result))*0.8)\n",
    "\n",
    "test_corpus_all = result[numbertraindata:]\n",
    "train_corpus_all = result[:numbertraindata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling\n",
    "train_corpus_all_irony = []\n",
    "train_corpus_all_noirony = []\n",
    "for entry in train_corpus_all:\n",
    "    if entry[1] == 1:\n",
    "        train_corpus_all_irony.append(entry)\n",
    "    else:\n",
    "        train_corpus_all_noirony.append(entry)\n",
    "count_irony_entries = len(train_corpus_all_irony)\n",
    "train_corpus_all = train_corpus_all_irony + train_corpus_all_noirony[:count_irony_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24844"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into text and labels\n",
    "def split_data(corpus):\n",
    "    data = []\n",
    "    label =[]\n",
    "    \n",
    "    for entry in corpus:\n",
    "        data.append(entry[0])\n",
    "        label.append(entry[1])\n",
    "    return data, label\n",
    "\n",
    "corpustrain_all,corpustrain_label  = split_data(train_corpus_all)\n",
    "corpustest_all, corpustest_label = split_data(test_corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "vocab_all = Counter()\n",
    "tweets_all = process_tweets(corpustrain_all + corpustest_all, vocab_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 1513\n"
     ]
    }
   ],
   "source": [
    "# Create train data, test data and dirctionary\n",
    "train_data_all, test_data_all, word_index_all = create_train_test_wordindex(vocab_all, tweets_all, corpustrain_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the train data, test data and word index for the German Irony Corpus\n",
    "write_file(file_all_test_data,test_data_all)\n",
    "write_file(file_all_test_labels,corpustest_label)\n",
    "write_file(file_all_train_data,train_data_all)\n",
    "write_file(file_all_train_labels,corpustrain_label)\n",
    "\n",
    "write_index(file_all_word_index,word_index_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all englisch Twitter data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_ALL = path + 'english_datasets_all_without_Ravi_and_Wallace.csv'\n",
    "\n",
    "file_all_train_data = output + 'english_twitter_train_data_balanced.csv'\n",
    "file_all_train_labels = output +'english_twitter_train_labels_balanced.csv'\n",
    "file_all_test_data = output +'english_twitter_test_data_balanced.csv'\n",
    "file_all_test_labels = output +'english_twitter_test_labels_balanced.csv'\n",
    "file_all_word_index = output +'english_twitter_word_index_balanced.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_all = read_data_for_combination(ENGLISH_ALL,2 ,1 ,'filename\t' , '\\t', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test data\n",
    "result = random.sample(list(corpus_all),len(corpus_all))\n",
    "numbertraindata = int((len(result))*0.8)\n",
    "\n",
    "test_corpus_all = result[numbertraindata:]\n",
    "train_corpus_all = result[:numbertraindata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling\n",
    "train_corpus_all_irony = []\n",
    "train_corpus_all_noirony = []\n",
    "for entry in train_corpus_all:\n",
    "    if entry[1] == 1:\n",
    "        train_corpus_all_irony.append(entry)\n",
    "    else:\n",
    "        train_corpus_all_noirony.append(entry)\n",
    "count_irony_entries = len(train_corpus_all_irony)\n",
    "train_corpus_all = train_corpus_all_irony + train_corpus_all_noirony[:count_irony_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23246"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into text and labels\n",
    "def split_data(corpus):\n",
    "    data = []\n",
    "    label =[]\n",
    "    \n",
    "    for entry in corpus:\n",
    "        data.append(entry[0])\n",
    "        label.append(entry[1])\n",
    "    return data, label\n",
    "\n",
    "corpustrain_all,corpustrain_label  = split_data(train_corpus_all)\n",
    "corpustest_all, corpustest_label = split_data(test_corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "vocab_all = Counter()\n",
    "tweets_all = process_tweets(corpustrain_all + corpustest_all, vocab_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 1747\n"
     ]
    }
   ],
   "source": [
    "# Create train data, test data and dirctionary\n",
    "train_data_all, test_data_all, word_index_all = create_train_test_wordindex(vocab_all, tweets_all, corpustrain_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the train data, test data and word index for the German Irony Corpus\n",
    "write_file(file_all_test_data,test_data_all)\n",
    "write_file(file_all_test_labels,corpustest_label)\n",
    "write_file(file_all_train_data,train_data_all)\n",
    "write_file(file_all_train_labels,corpustrain_label)\n",
    "\n",
    "write_index(file_all_word_index,word_index_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all english Comment data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_ALL = path + 'english_datasets_only_wallace_and_ravi.csv'\n",
    "\n",
    "file_all_train_data = output + 'english_comment_train_data_balanced.csv'\n",
    "file_all_train_labels = output +'english_comment_train_labels_balanced.csv'\n",
    "file_all_test_data = output +'english_comment_test_data_balanced.csv'\n",
    "file_all_test_labels = output +'english_comment_test_labels_balanced.csv'\n",
    "file_all_word_index = output +'english_comment_word_index_balanced.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_all = read_data_for_combination(ENGLISH_ALL,2 ,1 ,'filename\t' , '\\t', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test data\n",
    "result = random.sample(list(corpus_all),len(corpus_all))\n",
    "numbertraindata = int((len(result))*0.8)\n",
    "\n",
    "test_corpus_all = result[numbertraindata:]\n",
    "train_corpus_all = result[:numbertraindata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling\n",
    "train_corpus_all_irony = []\n",
    "train_corpus_all_noirony = []\n",
    "for entry in train_corpus_all:\n",
    "    if entry[1] == 1:\n",
    "        train_corpus_all_irony.append(entry)\n",
    "    else:\n",
    "        train_corpus_all_noirony.append(entry)\n",
    "count_irony_entries = len(train_corpus_all_irony)\n",
    "train_corpus_all = train_corpus_all_irony + train_corpus_all_noirony[:count_irony_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1694"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into text and labels\n",
    "def split_data(corpus):\n",
    "    data = []\n",
    "    label =[]\n",
    "    \n",
    "    for entry in corpus:\n",
    "        data.append(entry[0])\n",
    "        label.append(entry[1])\n",
    "    return data, label\n",
    "\n",
    "corpustrain_all,corpustrain_label  = split_data(train_corpus_all)\n",
    "corpustest_all, corpustest_label = split_data(test_corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "vocab_all = Counter()\n",
    "tweets_all = process_tweets(corpustrain_all + corpustest_all, vocab_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 1233\n"
     ]
    }
   ],
   "source": [
    "# Create train data, test data and dirctionary\n",
    "train_data_all, test_data_all, word_index_all = create_train_test_wordindex(vocab_all, tweets_all, corpustrain_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the train data, test data and word index for the German Irony Corpus\n",
    "write_file(file_all_test_data,corpustest_all)\n",
    "write_file(file_all_test_labels,corpustest_label)\n",
    "write_file(file_all_train_data,corpustrain_all)\n",
    "write_file(file_all_train_labels,corpustrain_label)\n",
    "\n",
    "write_index(file_all_word_index,word_index_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine SamEval and Reyes et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMEVAL = path + 'SemEval2018-T3-train-taskA_emoji.txt'\n",
    "TEST_SAMEVAL = path + 'SemEval2018-T3_gold_test_taskA_emoji.txt' \n",
    "IRONY = path + 'Irony.txt'\n",
    "EDUCATION = path + 'Education.txt'\n",
    "\n",
    "samevalcorpus = read_data_for_combination(TRAIN_SAMEVAL, 2, 1, 'tweet index', '\\t', True)\n",
    "samevalcorpustest = read_data_for_combination (TEST_SAMEVAL, 2, 1, 'tweet index', '\\t', True )\n",
    "reyes_irony_corpus = read_data_for_combination(IRONY, -1, 1,'#+no header+#',  '\\n', False)\n",
    "reyes_education_corpus = read_data_for_combination(EDUCATION, -1, 0,'#+no header+#','\\n', False  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine the train and test data from samEval with 10.000 data from irony corpus and 10.000 from education corpus from reyes at al.. Test data will be 20% of the whole data from all corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number data allthogether: 24414\n",
      "Number train data: 19531\n",
      "Number test data: 4883\n",
      "-----------\n",
      "Trainlabel distribution: Counter({1: 9848, 0: 9683})\n",
      "Testlabel distribution: Counter({0: 2545, 1: 2338})\n"
     ]
    }
   ],
   "source": [
    "corpuslist = []\n",
    "corpuslist.append(samevalcorpus)\n",
    "corpuslist.append(samevalcorpustest)\n",
    "corpuslist.append(reyes_irony_corpus)\n",
    "corpuslist.append(reyes_education_corpus)\n",
    "\n",
    "train, test = concatenate_train_test(corpuslist)\n",
    "\n",
    "print('Number data allthogether: '+str(len(train+test)))\n",
    "print('Number train data: '+str(len(train)))\n",
    "print('Number test data: '+str(len(test)))\n",
    "\n",
    "#next split\n",
    "traindata, trainlabels = split_text_label(train)\n",
    "testdata, testlabels = split_text_label(test)\n",
    "\n",
    "print('-----------')\n",
    "print('Trainlabel distribution: '+str(Counter(trainlabels)))\n",
    "print('Testlabel distribution: '+str(Counter(testlabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "vocab_sam_reyes_all = Counter()\n",
    "tweets_sam_reyes_all = process_tweets(traindata + testdata, vocab_sam_reyes_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 99\n"
     ]
    }
   ],
   "source": [
    "# Create train data, test data and dirctionary\n",
    "train_data_sam_reyes_all, test_data_sam_reyes_all, word_index_sam_reyes_all = create_train_test_wordindex(vocab_sam_reyes_all, tweets_sam_reyes_all, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store train data, test data and word index for the combination of samEval and Reyes et al.\n",
    "write_file(output + 'sam_reyes_all_train_data.csv', train_data_sam_reyes_all)\n",
    "write_file(output + 'sam_reyes_all_train_labels.csv', trainlabels)\n",
    "\n",
    "write_file(output + 'sam_reyes_all_test_data.csv', test_data_sam_reyes_all)\n",
    "write_file(output + 'sam_reyes_all_test_labels.csv', testlabels)\n",
    "\n",
    "write_index(output + 'sam_reyes_all_word_index.csv', word_index_sam_reyes_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of SamEval and Ravi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMEVAL = path + 'SemEval2018-T3-train-taskA_emoji_ironyHashtags.txt'\n",
    "TEST_SAMEVAL = path + 'SemEval2018-T3_gold_test_taskA_emoji.txt' \n",
    "RAVIIRONY = path + 'Ravi_finalpos499.txt'\n",
    "RAVINONIRONY = path + 'Ravi_finalNeg2498.csv'\n",
    "\n",
    "samevalcorpustrain = read_data_for_combination(TRAIN_SAMEVAL, 2, 1, 'tweet index', '\\t', True)\n",
    "samevalcorpustest = read_data_for_combination (TEST_SAMEVAL, 2, 1, 'tweet index', '\\t', True )\n",
    "ravi_irony = read_data_for_combination(RAVIIRONY,0, 1, '#+no header+#', '\\n', False )\n",
    "ravi_non_irony = read_data_for_combination(RAVINONIRONY,0, 0, '#+no header+#', '\\n', False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train data: 6092\n",
      "Number test data: 1523\n",
      "Label distribution train data: Counter({0: 3957, 1: 2135})\n",
      "Label distribution test data: Counter({0: 937, 1: 586})\n"
     ]
    }
   ],
   "source": [
    "corpuslist = []\n",
    "corpuslist.append(samevalcorpustrain)\n",
    "corpuslist.append(samevalcorpustest)\n",
    "corpuslist.append(ravi_irony)\n",
    "corpuslist.append(ravi_non_irony)\n",
    "\n",
    "train, test = concatenate_train_test(corpuslist)\n",
    "\n",
    "print('Number train data: '+str(len(train)))\n",
    "print('Number test data: '+str(len(test)))\n",
    "\n",
    "traindata, trainlabels = split_text_label(train)\n",
    "testdata, testlabels = split_text_label(test)\n",
    "\n",
    "print('Label distribution train data: '+str(Counter(trainlabels)))\n",
    "print('Label distribution test data: '+str(Counter(testlabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "vocab_sam_ravi = Counter()\n",
    "tweets_sam_ravi = process_tweets(traindata + testdata, vocab_sam_ravi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 1233\n"
     ]
    }
   ],
   "source": [
    "# Create train data, test data and dirctionary\n",
    "train_data_sam_ravi, test_data_sam_ravi, word_index_sam_ravi = create_train_test_wordindex(vocab_sam_ravi, tweets_sam_ravi, traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store train data, test data and word index for the combination of samEval and ravi corpus\n",
    "write_file(output + 'sam_ravi_train_data.csv', train_data_sam_ravi)\n",
    "write_file(output + 'sam_ravi_train_labels.csv', trainlabels)\n",
    "\n",
    "write_file(output + 'sam_ravi_test_data.csv', test_data_sam_ravi)\n",
    "write_file(output + 'sam_ravi_test_labels.csv', testlabels)\n",
    "\n",
    "write_index(output + 'sam_ravi_word_index.csv', word_index_sam_ravi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine SamEval and IronITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMEVAL = path + 'SemEval2018-T3-train-taskA_emoji_ironyHashtags.txt'\n",
    "TEST_SAMEVAL = path + 'SemEval2018-T3_gold_test_taskA_emoji.txt' \n",
    "\n",
    "TRAIN_IRONITA = path + 'training_ironita2018.csv'\n",
    "TEST_IRONITA  = path + 'test_gold_ironita2018.csv'\n",
    "\n",
    "samevalcorpustrain = read_data_for_combination(TRAIN_SAMEVAL, 2, 1, 'tweet index', '\\t', True)\n",
    "samevalcorpustest = read_data_for_combination (TEST_SAMEVAL, 2, 1, 'tweet index', '\\t', True )\n",
    "ironitacorpustrain = read_data_for_combination(TRAIN_IRONITA,1, 2, 'id', '\\t', True )\n",
    "ironitacorpustest = read_data_for_combination(TEST_IRONITA,1, 2, 'id','\\t', True  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train data: 7573\n",
      "Number test data: 1894\n",
      "Label distribution train data: Counter({0: 3791, 1: 3782})\n",
      "Label distribution test data: Counter({0: 996, 1: 898})\n"
     ]
    }
   ],
   "source": [
    "corpuslist = []\n",
    "corpuslist.append(samevalcorpustrain)\n",
    "corpuslist.append(samevalcorpustest)\n",
    "corpuslist.append(ironitacorpustrain)\n",
    "corpuslist.append(ironitacorpustest)\n",
    "\n",
    "train, test = concatenate_train_test(corpuslist)\n",
    "\n",
    "print('Number train data: '+str(len(train)))\n",
    "print('Number test data: '+str(len(test)))\n",
    "\n",
    "traindata, trainlabels = split_text_label(train)\n",
    "testdata, testlabels = split_text_label(test)\n",
    "\n",
    "print('Label distribution train data: '+str(Counter(trainlabels)))\n",
    "print('Label distribution test data: '+str(Counter(testlabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "vocab_sam_ironita = Counter()\n",
    "tweets_sam_ironita = process_tweets(traindata + testdata, vocab_sam_ironita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 40\n"
     ]
    }
   ],
   "source": [
    "# Create train data, test data and dirctionary\n",
    "train_data_sam_ironita, test_data_sam_ironita, word_index_sam_ironita = create_train_test_wordindex(vocab_sam_ironita, tweets_sam_ironita, traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store train data, test data and word index for the combination of samEval and IronITA\n",
    "write_file(output + 'sam_ironita_train_data.csv', train_data_sam_ironita)\n",
    "write_file(output + 'sam_ironita_train_labels.csv', trainlabels)\n",
    "\n",
    "write_file(output + 'sam_ironita_test_data.csv', test_data_sam_ironita)\n",
    "write_file(output + 'sam_ironita_test_labels.csv', testlabels)\n",
    "\n",
    "write_index(output + 'sam_ironita_word_index.csv', word_index_sam_ironita)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine SamEval and German data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test data are a combination of samEval and German corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMEVAL = path + 'SemEval2018-T3-train-taskA_emoji.txt'\n",
    "TEST_SAMEVAL = path + 'SemEval2018-T3_gold_test_taskA_emoji.txt' \n",
    "DATA_TETZNER = path + 'GermanIronyCorpus_cleaned.txt'\n",
    "\n",
    "samevalcorpustrain = read_data_for_combination(TRAIN_SAMEVAL, 2, 1, 'tweet index', '\\t', True)\n",
    "samevalcorpustest = read_data_for_combination (TEST_SAMEVAL, 2, 1, 'tweet index', '\\t', True )\n",
    "germancorpus = read_data_for_combination(DATA_TETZNER,0, 4, '#+no header+#', '\\t', True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number train data: 7134\n",
      "Number test data: 1784\n",
      "Label distribution train data: Counter({0: 4518, 1: 2616})\n",
      "Label distribution test data: Counter({0: 1091, 1: 693})\n"
     ]
    }
   ],
   "source": [
    "corpuslist = []\n",
    "corpuslist.append(samevalcorpustrain)\n",
    "corpuslist.append(samevalcorpustest)\n",
    "corpuslist.append(germancorpus)\n",
    "\n",
    "train, test = concatenate_train_test(corpuslist)\n",
    "\n",
    "print('Number train data: '+str(len(train)))\n",
    "print('Number test data: '+str(len(test)))\n",
    "\n",
    "traindata, trainlabels = split_text_label(train)\n",
    "testdata, testlabels = split_text_label(test)\n",
    "\n",
    "print('Label distribution train data: '+str(Counter(trainlabels)))\n",
    "print('Label distribution test data: '+str(Counter(testlabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "vocab_sam_german = Counter()\n",
    "tweets_sam_german = process_tweets(traindata + testdata, vocab_sam_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 73\n"
     ]
    }
   ],
   "source": [
    "# Create train data, test data and dirctionary\n",
    "train_data_sam_german, test_data_sam_german, word_index_sam_german = create_train_test_wordindex(vocab_sam_german, tweets_sam_german, traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store train data, test data and word index for the combination of samEval and GErman corpus\n",
    "write_file(output + 'sam_german_train_data.csv', train_data_sam_german)\n",
    "write_file(output + 'sam_german_train_labels.csv', trainlabels)\n",
    "\n",
    "write_file(output + 'sam_german_test_data.csv', test_data_sam_german)\n",
    "write_file(output + 'sam_german_test_labels.csv', testlabels)\n",
    "\n",
    "write_index(output + 'sam_german_word_index.csv', word_index_sam_german)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
