{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "import random as rd\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from textblob import TextBlob\n",
    "\n",
    "from keras.layers import Dense, Embedding, Flatten, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adagrad\n",
    "\n",
    "#Import another jupyter notebook\n",
    "import import_ipynb\n",
    "from Productive_CrossValidation_inclTest import *\n",
    "from Productive_Data_GetTrainTest import samEval_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproducability of the results\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "rd.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "output= 'output_trainTestData/'\n",
    "outputpath= 'outputs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read SamEval2018 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Had to be adopted for new data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMEVAL = path + 'SemEval2018-T3-train-taskA_emoji_ironyHashtags.txt'\n",
    "GOLD_TEST_SAMEVAL  = path + 'SemEval2018-T3_gold_test_taskA_emoji.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "def text_to_wordlist(tweet, vocab):\n",
    "    #Remove hashtags\n",
    "    #tweet = re.sub('#','', tweet)\n",
    "    \n",
    "    #Remove usermentions\n",
    "    tweet= re.sub(r'(\\w+|^|)@\\w+','', tweet)\n",
    "    \n",
    "    #Treats url's as special tokens (actually twitter specific)\n",
    "    tweet=re.sub(r'((http|https)://)(\\w|[.]|/)+', 'URL', tweet)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    vocab.update(tweet)\n",
    "    return tweet\n",
    "\n",
    "def process_tweets(list_sentences, vocab):\n",
    "    tweets = []\n",
    "    for tweet in list_sentences:\n",
    "        twt = text_to_wordlist(tweet, vocab)\n",
    "        tweets.append(twt)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train data, test data and word_index\n",
    "def create_train_test_wordindex(vocab, tweets, corpustrain):\n",
    "    MAX_NB_WORDS = len(vocab)\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH= 0\n",
    "    for tweet in tweets:\n",
    "        if len(tweet) > MAX_SEQUENCE_LENGTH:\n",
    "            MAX_SEQUENCE_LENGTH = len(tweet)\n",
    "            \n",
    "    print('Maximal Sequence Length: '+str(MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n",
    "\n",
    "    sequences = [[word_index.get(t, 0) for t in tweet]\n",
    "                 for tweet in tweets[:len(corpustrain)]]\n",
    "\n",
    "    test_sequences = [[word_index.get(t, 0) for t in tweet] \n",
    "                      for tweet in tweets[len(corpustrain):]]\n",
    "\n",
    "    train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                         padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "    test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n",
    "                              truncating=\"post\")\n",
    "\n",
    "    return train_data, test_data, word_index, MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_samEval(FILENAME):\n",
    "    labels = []\n",
    "    corpus = []   \n",
    "    with open(FILENAME, 'r', encoding='utf-8') as train: #Decoding utf-8, else exception\n",
    "        for line in train:\n",
    "            if not line.lower().startswith(\"tweet index\"): #skip header\n",
    "                line = line.rstrip().split(\"\\t\")\n",
    "                label = line[1] #erste Spalte - label\n",
    "                labels.append(int(label))\n",
    "                tweet = line[2] #zweite Spalte - tweet\n",
    "                corpus.append(tweet)  \n",
    "    returnvalue = []\n",
    "    returnvalue.append(labels)\n",
    "    returnvalue.append(corpus)\n",
    "    return returnvalue\n",
    "\n",
    "resulttrain = read_data_samEval(TRAIN_SAMEVAL)\n",
    "labelstrain_sameval = resulttrain[0]\n",
    "corpustrain_sameval = resulttrain[1]\n",
    "\n",
    "resulttest = read_data_samEval(GOLD_TEST_SAMEVAL)\n",
    "labelstest_sameval = resulttest[0]\n",
    "corpustest_sameval = resulttest[1]\n",
    "\n",
    "#Prepocess the data\n",
    "vocab_sameval = Counter()\n",
    "tweets_sameval = process_tweets(corpustrain_sameval + corpustest_sameval, vocab_sameval)\n",
    "\n",
    "train_data, test_data, word_index, max_len = create_train_test_wordindex(vocab_sameval, tweets_sameval, corpustrain_sameval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create POS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_sequence(data, MAXLEN):\n",
    "         \n",
    "    pos = []\n",
    "    pos_list = []\n",
    "    for entry in data:\n",
    "        postags = nltk.pos_tag(entry)\n",
    "        for tag in postags:\n",
    "            if tag[1] not in pos_list:\n",
    "                pos_list.append(tag[1])\n",
    "        pos.append(postags)\n",
    "        \n",
    "    pos_dict ={}\n",
    "    i = 1\n",
    "\n",
    "    for entry in pos_list:\n",
    "        pos_dict[entry] = i\n",
    "        i += 1\n",
    "        \n",
    "    decoded_pos =[]\n",
    "    for entry in pos:\n",
    "        tweet = []\n",
    "        for tup in entry:\n",
    "            decode = pos_dict.get(tup[1])\n",
    "            tweet.append(decode)\n",
    "        decoded_pos.append(tweet)\n",
    "        \n",
    "    pos_sequences = pad_sequences(decoded_pos, maxlen=MAXLEN,padding=\"pre\", truncating=\"post\")\n",
    "    \n",
    "    return pos_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = create_pos_sequence(tweets_sameval[:len(corpustrain_sameval)], max_len)\n",
    "test_pos = create_pos_sequence(tweets_sameval[len(corpustrain_sameval):], max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_sentiment(data, MAXLEN):\n",
    "    sentiment_blob = []\n",
    "\n",
    "    for entry in data:\n",
    "        sentiment_line = []\n",
    "        for word in entry:\n",
    "            word = TextBlob(word)\n",
    "            polarity = word.sentiment.polarity\n",
    "            sentiment_line.append(polarity)\n",
    "        sentiment_blob.append(sentiment_line)\n",
    "        \n",
    "    pad_data= pad_sequences(sentiment_blob, maxlen=MAXLEN,padding=\"pre\", truncating=\"post\", dtype='float32')\n",
    "    return pad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_senti_blob = blob_sentiment(tweets_sameval[:len(corpustrain_sameval)], max_len)\n",
    "test_senti_blob = blob_sentiment(tweets_sameval[len(corpustrain_sameval):], max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pretrained embeddings\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Pretrained Google News Embeddings, Dimension 300\n",
    "GOOGLEEMB = path + 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "embedding= KeyedVectors.load_word2vec_format(GOOGLEEMB, binary=True)\n",
    "\n",
    "word_vectors = embedding.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create embedding-matrix, serves the Keras Embedding Layer as weigths\n",
    "EMBEDDING_DIM=300\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector   \n",
    "    except:\n",
    "        # words not found in embeddings will be zero\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 30\n",
    "softmax = True\n",
    "lossfunction ='binary_crossentropy'\n",
    "optimizer = Adagrad(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model\n",
    "wv_layer = Embedding(len(word_index)+1,\n",
    "                 EMBEDDING_DIM,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=max_len,\n",
    "                 trainable=True)\n",
    "\n",
    "comment_input = Input(shape=(max_len,), dtype='float32')\n",
    "embedded_sequences = wv_layer(comment_input)\n",
    "x_first = Dense(600, activation='relu')(embedded_sequences)\n",
    "x_first = Flatten()(x_first)\n",
    "\n",
    "#Second part\n",
    "senti_input_blob = Input(shape=(max_len,), dtype='float')\n",
    "x_second = Dense(600, activation='relu')(senti_input_blob)\n",
    "\n",
    "#Thrid part\n",
    "senti_input_pos = Input(shape=(max_len,), dtype='float')\n",
    "x_third = Dense(600, activation='relu')(senti_input_pos)\n",
    "\n",
    "#Concatination\n",
    "concat = concatenate([x_first, x_second, x_third])\n",
    "\n",
    "preds = Dense(2, activation='softmax')(concat)\n",
    "\n",
    "model = Model(inputs=[comment_input, senti_input_blob, senti_input_pos], outputs=preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_three_inputs(model, NUM_EPOCHS, BATCH_SIZE, optimizer, lossfunction, train_data, train_senti_blob, train_pos, labelstrain_sameval, softmax, outputpath, 'samEvalCrosval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model with the heldout test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_three_inputs(model, NUM_EPOCHS, BATCH_SIZE, optimizer, lossfunction, train_data, train_senti_blob, train_pos, labelstrain_sameval, test_data, test_senti_blob, test_pos, labelstest_sameval, softmax, outputpath, 'SamEval')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
