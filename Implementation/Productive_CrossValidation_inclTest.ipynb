{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rd\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import keras_metrics as km \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import clone_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import import_ipynb\n",
    "from Productive_TimeHistory import TimeHistory\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def cross_val(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_labels, softmax, outputpath, outputname):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "\n",
    "    #Have to convert the data to an np.arry, to use the indices generated by StratifiedKFold\n",
    "    crossvaldata = np.array(train_data)\n",
    "    crossvallabels = np.array(labelstrain)\n",
    "\n",
    "    #Define 10-fold cross validation test harness with StratifiedKFold\n",
    "    skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "    #Define variabels to store the scores for every fold\n",
    "    accscores = []\n",
    "    precisionscores = []\n",
    "    recallscores = []\n",
    "    f1scores = []\n",
    "    \n",
    "\n",
    "    #Fold number, needed for stoing to csv\n",
    "    nrfold = 1\n",
    "\n",
    "    #Cross validation\n",
    "    for train, test in skfold.split(train_data, train_labels):\n",
    "        time_callback = TimeHistory() \n",
    "        \n",
    "        foldmodel = clone_model(model)\n",
    "        \n",
    "        #evaluate the positive class (=ironic data)\n",
    "        precision = km.binary_precision(label=1)\n",
    "        recall = km.binary_recall(label=1) \n",
    "        f1 = km.f1_score(label=1)\n",
    "        \n",
    "        foldmodel.compile(loss = lossfunction,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy',precision, recall, f1])\n",
    "\n",
    "        #Train the model with validation data splited through cross validation\n",
    "        hist= foldmodel.fit(crossvaldata[train], crossvallabels[train], epochs=nrepochs, batch_size=nrbatch, \n",
    "                            validation_data=(crossvaldata[test], crossvallabels[test]), shuffle=False, verbose = 0, callbacks=[time_callback])\n",
    "\n",
    "        #Append average of the validation scores for the current fold\n",
    "        accscores.append(np.mean(hist.history['val_acc'])*100)\n",
    "        precisionscores.append(np.mean(hist.history['val_precision'])*100)\n",
    "        recallscores.append(np.mean(hist.history['val_recall'])*100)\n",
    "        f1scores.append(np.mean(hist.history['val_f1_score'])*100)\n",
    "\n",
    "        print(\"\\n-----Fold \"+str(nrfold)+\"--------\")\n",
    "        \n",
    "        #Plot the metrics for every epoch for the current fold\n",
    "        from matplotlib import pyplot\n",
    "        %matplotlib inline\n",
    "        pyplot.figure(figsize=(15,10))\n",
    "        pyplot.plot(hist.history['acc']) \n",
    "        pyplot.plot(hist.history['f1_score'])\n",
    "        pyplot.plot(hist.history['val_acc']) \n",
    "        pyplot.plot(hist.history['val_f1_score'])\n",
    "        #pyplot.plot(hist.history['loss'])\n",
    "        pyplot.xticks(np.arange(1,nrepochs))\n",
    "        pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "        pyplot.show()\n",
    "        \n",
    "        #Print the average validation scores for the current fold\n",
    "        print(\"Average Accuracy: %.2f%%\" % ((np.mean(hist.history['val_acc']))*100))\n",
    "        print(\"Average Precision: %.2f%%\" % ((np.mean(hist.history['val_precision']))*100))\n",
    "        print(\"Average Recall: %.2f%%\" % ((np.mean(hist.history['val_recall']))*100))\n",
    "        print(\"Average F1: %.2f%%\" % ((np.mean(hist.history['val_f1_score']))*100))\n",
    "\n",
    "        #Store the keras model\n",
    "        foldmodel.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                       '_'+str(nrfold)+'_model.h5')\n",
    "\n",
    "        #Store the train and validation scores for every epoch for the current fold to csv + train time\n",
    "        f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+'_'+\n",
    "                str(nrfold)+'_train_val_scores.csv','w+')\n",
    "        f.write('Epoch; Loss; Accuracy; Precision; Recall; F1; Val_loss; Val_Accuracy; Val_Precision; VAl_Recall; Val_F1; Time \\n')\n",
    "\n",
    "        for i in range(len(hist.history['acc'])):\n",
    "            f.write(str(i+1)+';'+str(hist.history['loss'][i])+';'+str(hist.history['acc'][i])+';'+\n",
    "                    str(hist.history['precision'][i])+';'+str(hist.history['recall'][i])+';'+\n",
    "                    str(hist.history['f1_score'][i])+';'+str(hist.history['val_loss'][i])+';'+\n",
    "                    str(hist.history['val_acc'][i])+';'+str(hist.history['val_precision'][i])+';'+\n",
    "                    str(hist.history['val_recall'][i])+';'+str(hist.history['val_f1_score'][i])+';'+\n",
    "                    str(time_callback.times[i])+'\\n')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        nrfold+=1\n",
    "      \n",
    "    #Plot and print the results of all folds \n",
    "    print(\"\\n-------Overallresults-------\")\n",
    "\n",
    "    #Plot the overall Metrics\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(accscores) \n",
    "    pyplot.plot(precisionscores)\n",
    "    pyplot.plot(recallscores)\n",
    "    pyplot.plot(f1scores)\n",
    "    #pyplot.xticks(np.arange(1,nrfold))\n",
    "    pyplot.legend(['Accuracy', 'Precision', 'Recall', 'F1'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Print mean, min and max values\n",
    "    print(\"Accuracy: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(accscores), np.std(accscores), np.max(accscores), np.min(accscores)))\n",
    "    print(\"Precision: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(precisionscores), np.std(precisionscores), np.max(precisionscores), np.min(precisionscores)))\n",
    "    print(\"Recall: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(recallscores), np.std(recallscores), np.max(recallscores), np.min(recallscores)))\n",
    "    print(\"F1: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(f1scores), np.std(f1scores), np.max(f1scores), np.min(f1scores)))\n",
    "    \n",
    "    #Write validation scores for every fold to csv\n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+\n",
    "            '_validationscores.csv',\"w+\")\n",
    "    f.write('Accuracy; Precision; Recall; F1 \\n')\n",
    "    for i in range(len(accscores)):\n",
    "        f.write(str(accscores[i])+';'+str(precisionscores[i])+';'+str(recallscores[i])+';'+str(f1scores[i])+'\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def cross_val_with_earlystopping(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_labels, \n",
    "                                 softmax, outputpath, outputname, es_monitor, es_patience):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "\n",
    "    #Have to convert the data to an np.arry, to use the indices generated by StratifiedKFold\n",
    "    crossvaldata = np.array(train_data)\n",
    "    crossvallabels = np.array(labelstrain)\n",
    "\n",
    "    #Define 10-fold cross validation test harness with StratifiedKFold\n",
    "    skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "    #Define variabels to store the scores for every fold\n",
    "    accscores = []\n",
    "    precisionscores = []\n",
    "    recallscores = []\n",
    "    f1scores = []\n",
    "    \n",
    "\n",
    "    #Fold number, needed for stoing to csv\n",
    "    nrfold = 1\n",
    "\n",
    "    #Cross validation\n",
    "    for train, test in skfold.split(train_data, train_labels):\n",
    "        time_callback = TimeHistory() \n",
    "        \n",
    "        es = EarlyStopping(monitor=es_monitor,\n",
    "                              min_delta=0,\n",
    "                              patience=es_patience,\n",
    "                              verbose=0, mode='auto')\n",
    "        \n",
    "        foldmodel = clone_model(model)\n",
    "        \n",
    "        #evaluate the positive class (=ironic data)\n",
    "        precision = km.binary_precision(label=1)\n",
    "        recall = km.binary_recall(label=1) \n",
    "        f1 = km.f1_score(label=1)\n",
    "        \n",
    "        foldmodel.compile(loss = lossfunction,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy',precision, recall, f1])\n",
    "\n",
    "        #Train the model with validation data splited through cross validation\n",
    "        hist= foldmodel.fit(crossvaldata[train], crossvallabels[train], epochs=nrepochs, batch_size=nrbatch, \n",
    "                            validation_data=(crossvaldata[test], crossvallabels[test]), shuffle=False, verbose = 1, callbacks=[es, time_callback])\n",
    "\n",
    "        #Append average of the validation scores for the current fold\n",
    "        accscores.append(np.mean(hist.history['val_acc'])*100)\n",
    "        precisionscores.append(np.mean(hist.history['val_precision'])*100)\n",
    "        recallscores.append(np.mean(hist.history['val_recall'])*100)\n",
    "        f1scores.append(np.mean(hist.history['val_f1_score'])*100)\n",
    "\n",
    "        print(\"\\n-----Fold \"+str(nrfold)+\"--------\")\n",
    "        \n",
    "        #Plot the metrics for every epoch for the current fold\n",
    "        from matplotlib import pyplot\n",
    "        %matplotlib inline\n",
    "        pyplot.figure(figsize=(15,10))\n",
    "        pyplot.plot(hist.history['acc']) \n",
    "        pyplot.plot(hist.history['f1_score'])\n",
    "        pyplot.plot(hist.history['val_acc']) \n",
    "        pyplot.plot(hist.history['val_f1_score'])\n",
    "        #pyplot.plot(hist.history['loss'])\n",
    "        pyplot.xticks(np.arange(1,nrepochs))\n",
    "        pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "        pyplot.show()\n",
    "        \n",
    "        #Print the average validation scores for the current fold\n",
    "        print(\"Average Accuracy: %.2f%%\" % ((np.mean(hist.history['val_acc']))*100))\n",
    "        print(\"Average Precision: %.2f%%\" % ((np.mean(hist.history['val_precision']))*100))\n",
    "        print(\"Average Recall: %.2f%%\" % ((np.mean(hist.history['val_recall']))*100))\n",
    "        print(\"Average F1: %.2f%%\" % ((np.mean(hist.history['val_f1_score']))*100))\n",
    "\n",
    "        #Store the keras model\n",
    "        foldmodel.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                       '_'+str(nrfold)+'_model.h5')\n",
    "\n",
    "        #Store the train and validation scores for every epoch for the current fold to csv + train time\n",
    "        f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+'_'+\n",
    "                str(nrfold)+'_train_val_scores.csv','w+')\n",
    "        f.write('Epoch; Loss; Accuracy; Precision; Recall; F1; Val_loss; Val_Accuracy; Val_Precision; VAl_Recall; Val_F1; Time \\n')\n",
    "\n",
    "        for i in range(len(hist.history['acc'])):\n",
    "            f.write(str(i+1)+';'+str(hist.history['loss'][i])+';'+str(hist.history['acc'][i])+';'+\n",
    "                    str(hist.history['precision'][i])+';'+str(hist.history['recall'][i])+';'+\n",
    "                    str(hist.history['f1_score'][i])+';'+str(hist.history['val_loss'][i])+';'+\n",
    "                    str(hist.history['val_acc'][i])+';'+str(hist.history['val_precision'][i])+';'+\n",
    "                    str(hist.history['val_recall'][i])+';'+str(hist.history['val_f1_score'][i])+';'+\n",
    "                    str(time_callback.times[i])+'\\n')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        nrfold+=1\n",
    "      \n",
    "    #Plot and print the results of all folds \n",
    "    print(\"\\n-------Overallresults-------\")\n",
    "\n",
    "    #Plot the overall Metrics\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(accscores) \n",
    "    pyplot.plot(precisionscores)\n",
    "    pyplot.plot(recallscores)\n",
    "    pyplot.plot(f1scores)\n",
    "    #pyplot.xticks(np.arange(1,nrfold))\n",
    "    pyplot.legend(['Accuracy', 'Precision', 'Recall', 'F1'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Print mean, min and max values\n",
    "    print(\"Accuracy: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(accscores), np.std(accscores), np.max(accscores), np.min(accscores)))\n",
    "    print(\"Precision: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(precisionscores), np.std(precisionscores), np.max(precisionscores), np.min(precisionscores)))\n",
    "    print(\"Recall: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(recallscores), np.std(recallscores), np.max(recallscores), np.min(recallscores)))\n",
    "    print(\"F1: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(f1scores), np.std(f1scores), np.max(f1scores), np.min(f1scores)))\n",
    "    \n",
    "    #Write validation scores for every fold to csv\n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+\n",
    "            '_validationscores.csv',\"w+\")\n",
    "    f.write('Accuracy; Precision; Recall; F1 \\n')\n",
    "    for i in range(len(accscores)):\n",
    "        f.write(str(accscores[i])+';'+str(precisionscores[i])+';'+str(recallscores[i])+';'+str(f1scores[i])+'\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def cross_val_two_inputs(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_data2, train_labels, softmax, outputpath, outputname):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "\n",
    "    #Have to convert the data to an np.arry, to use the indices generated by StratifiedKFold\n",
    "    crossvaldata = np.array(train_data)\n",
    "    crossvaldata2 = np.array(train_data2)\n",
    "    crossvallabels = np.array(labelstrain)\n",
    "\n",
    "    #Define 10-fold cross validation test harness with StratifiedKFold\n",
    "    skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "    #Define variabels to store the scores for every fold\n",
    "    accscores = []\n",
    "    precisionscores = []\n",
    "    recallscores = []\n",
    "    f1scores = []\n",
    "    \n",
    "\n",
    "    #Fold number, needed for stoing to csv\n",
    "    nrfold = 1\n",
    "\n",
    "    #Cross validation\n",
    "    for train, test in skfold.split(train_data, train_labels):\n",
    "        time_callback = TimeHistory() \n",
    "        \n",
    "        foldmodel = clone_model(model)\n",
    "        \n",
    "        #evaluate the positive class (=ironic data)\n",
    "        precision = km.binary_precision(label=1)\n",
    "        recall = km.binary_recall(label=1) \n",
    "        f1 = km.f1_score(label=1)\n",
    "        \n",
    "        foldmodel.compile(loss = lossfunction,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy',precision, recall, f1])\n",
    "\n",
    "        #Train the model with validation data splited through cross validation\n",
    "        hist= foldmodel.fit([crossvaldata[train], crossvaldata2[train]], crossvallabels[train], epochs=nrepochs, batch_size=nrbatch, \n",
    "                            validation_data=([crossvaldata[test],crossvaldata2[test]], crossvallabels[test]), shuffle=False, verbose = 0, callbacks=[time_callback])\n",
    "\n",
    "        #Append average of the validation scores for the current fold\n",
    "        accscores.append(np.mean(hist.history['val_acc'])*100)\n",
    "        precisionscores.append(np.mean(hist.history['val_precision'])*100)\n",
    "        recallscores.append(np.mean(hist.history['val_recall'])*100)\n",
    "        f1scores.append(np.mean(hist.history['val_f1_score'])*100)\n",
    "\n",
    "        print(\"\\n-----Fold \"+str(nrfold)+\"--------\")\n",
    "        \n",
    "        #Plot the metrics for every epoch for the current fold\n",
    "        from matplotlib import pyplot\n",
    "        %matplotlib inline\n",
    "        pyplot.figure(figsize=(15,10))\n",
    "        pyplot.plot(hist.history['acc']) \n",
    "        pyplot.plot(hist.history['f1_score'])\n",
    "        pyplot.plot(hist.history['val_acc']) \n",
    "        pyplot.plot(hist.history['val_f1_score'])\n",
    "        #pyplot.plot(hist.history['loss'])\n",
    "        pyplot.xticks(np.arange(1,nrepochs))\n",
    "        pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "        pyplot.show()\n",
    "        \n",
    "        #Print the average validation scores for the current fold\n",
    "        print(\"Average Accuracy: %.2f%%\" % ((np.mean(hist.history['val_acc']))*100))\n",
    "        print(\"Average Precision: %.2f%%\" % ((np.mean(hist.history['val_precision']))*100))\n",
    "        print(\"Average Recall: %.2f%%\" % ((np.mean(hist.history['val_recall']))*100))\n",
    "        print(\"Average F1: %.2f%%\" % ((np.mean(hist.history['val_f1_score']))*100))\n",
    "\n",
    "        #Store the keras model\n",
    "        foldmodel.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                       '_'+str(nrfold)+'_model.h5')\n",
    "\n",
    "        #Store the train and validation scores for every epoch for the current fold to csv + train time\n",
    "        f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+'_'+\n",
    "                str(nrfold)+'_train_val_scores.csv','w+')\n",
    "        f.write('Epoch; Loss; Accuracy; Precision; Recall; F1; Val_loss; Val_Accuracy; Val_Precision; VAl_Recall; Val_F1; Time \\n')\n",
    "\n",
    "        for i in range(len(hist.history['acc'])):\n",
    "            f.write(str(i+1)+';'+str(hist.history['loss'][i])+';'+str(hist.history['acc'][i])+';'+\n",
    "                    str(hist.history['precision'][i])+';'+str(hist.history['recall'][i])+';'+\n",
    "                    str(hist.history['f1_score'][i])+';'+str(hist.history['val_loss'][i])+';'+\n",
    "                    str(hist.history['val_acc'][i])+';'+str(hist.history['val_precision'][i])+';'+\n",
    "                    str(hist.history['val_recall'][i])+';'+str(hist.history['val_f1_score'][i])+';'+\n",
    "                    str(time_callback.times[i])+'\\n')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        nrfold+=1\n",
    "      \n",
    "    #Plot and print the results of all folds \n",
    "    print(\"\\n-------Overallresults-------\")\n",
    "\n",
    "    #Plot the overall Metrics\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(accscores) \n",
    "    pyplot.plot(precisionscores)\n",
    "    pyplot.plot(recallscores)\n",
    "    pyplot.plot(f1scores)\n",
    "    #pyplot.xticks(np.arange(1,nrfold))\n",
    "    pyplot.legend(['Accuracy', 'Precision', 'Recall', 'F1'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Print mean, min and max values\n",
    "    print(\"Accuracy: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(accscores), np.std(accscores), np.max(accscores), np.min(accscores)))\n",
    "    print(\"Precision: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(precisionscores), np.std(precisionscores), np.max(precisionscores), np.min(precisionscores)))\n",
    "    print(\"Recall: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(recallscores), np.std(recallscores), np.max(recallscores), np.min(recallscores)))\n",
    "    print(\"F1: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(f1scores), np.std(f1scores), np.max(f1scores), np.min(f1scores)))\n",
    "    \n",
    "    #Write validation scores for every fold to csv\n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+\n",
    "            '_validationscores.csv',\"w+\")\n",
    "    f.write('Accuracy; Precision; Recall; F1 \\n')\n",
    "    for i in range(len(accscores)):\n",
    "        f.write(str(accscores[i])+';'+str(precisionscores[i])+';'+str(recallscores[i])+';'+str(f1scores[i])+'\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def cross_val_three_inputs(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_data2, train_data3,\n",
    "                           train_labels, softmax, outputpath, outputname):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "\n",
    "    #Have to convert the data to an np.arry, to use the indices generated by StratifiedKFold\n",
    "    crossvaldata = np.array(train_data)\n",
    "    crossvaldata2 = np.array(train_data2)\n",
    "    crossvaldata3 = np.array(train_data3)\n",
    "    crossvallabels = np.array(labelstrain)\n",
    "\n",
    "    #Define 10-fold cross validation test harness with StratifiedKFold\n",
    "    skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "    #Define variabels to store the scores for every fold\n",
    "    accscores = []\n",
    "    precisionscores = []\n",
    "    recallscores = []\n",
    "    f1scores = []\n",
    "    \n",
    "\n",
    "    #Fold number, needed for stoing to csv\n",
    "    nrfold = 1\n",
    "\n",
    "    #Cross validation\n",
    "    for train, test in skfold.split(train_data, train_labels):\n",
    "        time_callback = TimeHistory() \n",
    "        \n",
    "        foldmodel = clone_model(model)\n",
    "        \n",
    "        #evaluate the positive class (=ironic data)\n",
    "        precision = km.binary_precision(label=1)\n",
    "        recall = km.binary_recall(label=1) \n",
    "        f1 = km.f1_score(label=1)\n",
    "        \n",
    "        foldmodel.compile(loss = lossfunction,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy',precision, recall, f1])\n",
    "\n",
    "        #Train the model with validation data splited through cross validation\n",
    "        hist= foldmodel.fit([crossvaldata[train], crossvaldata2[train],  crossvaldata3[train]], crossvallabels[train], epochs=nrepochs, batch_size=nrbatch, \n",
    "                            validation_data=([crossvaldata[test],crossvaldata2[test], crossvaldata3[test]], \n",
    "                            crossvallabels[test]), shuffle=False, verbose = 0, callbacks=[time_callback])\n",
    "\n",
    "        #Append average of the validation scores for the current fold\n",
    "        accscores.append(np.mean(hist.history['val_acc'])*100)\n",
    "        precisionscores.append(np.mean(hist.history['val_precision'])*100)\n",
    "        recallscores.append(np.mean(hist.history['val_recall'])*100)\n",
    "        f1scores.append(np.mean(hist.history['val_f1_score'])*100)\n",
    "\n",
    "        print(\"\\n-----Fold \"+str(nrfold)+\"--------\")\n",
    "        \n",
    "        #Plot the metrics for every epoch for the current fold\n",
    "        from matplotlib import pyplot\n",
    "        %matplotlib inline\n",
    "        pyplot.figure(figsize=(15,10))\n",
    "        pyplot.plot(hist.history['acc']) \n",
    "        pyplot.plot(hist.history['f1_score'])\n",
    "        pyplot.plot(hist.history['val_acc']) \n",
    "        pyplot.plot(hist.history['val_f1_score'])\n",
    "        #pyplot.plot(hist.history['loss'])\n",
    "        pyplot.xticks(np.arange(1,nrepochs))\n",
    "        pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "        pyplot.show()\n",
    "        \n",
    "        #Print the average validation scores for the current fold\n",
    "        print(\"Average Accuracy: %.2f%%\" % ((np.mean(hist.history['val_acc']))*100))\n",
    "        print(\"Average Precision: %.2f%%\" % ((np.mean(hist.history['val_precision']))*100))\n",
    "        print(\"Average Recall: %.2f%%\" % ((np.mean(hist.history['val_recall']))*100))\n",
    "        print(\"Average F1: %.2f%%\" % ((np.mean(hist.history['val_f1_score']))*100))\n",
    "\n",
    "        #Store the keras model\n",
    "        foldmodel.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                       '_'+str(nrfold)+'_model.h5')\n",
    "\n",
    "        #Store the train and validation scores for every epoch for the current fold to csv + train time\n",
    "        f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+'_'+\n",
    "                str(nrfold)+'_train_val_scores.csv','w+')\n",
    "        f.write('Epoch; Loss; Accuracy; Precision; Recall; F1; Val_loss; Val_Accuracy; Val_Precision; VAl_Recall; Val_F1; Time \\n')\n",
    "\n",
    "        for i in range(len(hist.history['acc'])):\n",
    "            f.write(str(i+1)+';'+str(hist.history['loss'][i])+';'+str(hist.history['acc'][i])+';'+\n",
    "                    str(hist.history['precision'][i])+';'+str(hist.history['recall'][i])+';'+\n",
    "                    str(hist.history['f1_score'][i])+';'+str(hist.history['val_loss'][i])+';'+\n",
    "                    str(hist.history['val_acc'][i])+';'+str(hist.history['val_precision'][i])+';'+\n",
    "                    str(hist.history['val_recall'][i])+';'+str(hist.history['val_f1_score'][i])+';'+\n",
    "                    str(time_callback.times[i])+'\\n')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        nrfold+=1\n",
    "      \n",
    "    #Plot and print the results of all folds \n",
    "    print(\"\\n-------Overallresults-------\")\n",
    "\n",
    "    #Plot the overall Metrics\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(accscores) \n",
    "    pyplot.plot(precisionscores)\n",
    "    pyplot.plot(recallscores)\n",
    "    pyplot.plot(f1scores)\n",
    "    #pyplot.xticks(np.arange(1,nrfold))\n",
    "    pyplot.legend(['Accuracy', 'Precision', 'Recall', 'F1'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Print mean, min and max values\n",
    "    print(\"Accuracy: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(accscores), np.std(accscores), np.max(accscores), np.min(accscores)))\n",
    "    print(\"Precision: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(precisionscores), np.std(precisionscores), np.max(precisionscores), np.min(precisionscores)))\n",
    "    print(\"Recall: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(recallscores), np.std(recallscores), np.max(recallscores), np.min(recallscores)))\n",
    "    print(\"F1: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(f1scores), np.std(f1scores), np.max(f1scores), np.min(f1scores)))\n",
    "    \n",
    "    #Write validation scores for every fold to csv\n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+\n",
    "            '_validationscores.csv',\"w+\")\n",
    "    f.write('Accuracy; Precision; Recall; F1 \\n')\n",
    "    for i in range(len(accscores)):\n",
    "        f.write(str(accscores[i])+';'+str(precisionscores[i])+';'+str(recallscores[i])+';'+str(f1scores[i])+'\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def cross_val_four_inputs(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_data2, train_data3, train_data4,\n",
    "                           train_labels, softmax, outputpath, outputname):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "\n",
    "    #Have to convert the data to an np.arry, to use the indices generated by StratifiedKFold\n",
    "    crossvaldata = np.array(train_data)\n",
    "    crossvaldata2 = np.array(train_data2)\n",
    "    crossvaldata3 = np.array(train_data3)\n",
    "    crossvaldata4 = np.array(train_data4)\n",
    "    crossvallabels = np.array(labelstrain)\n",
    "\n",
    "    #Define 10-fold cross validation test harness with StratifiedKFold\n",
    "    skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "    #Define variabels to store the scores for every fold\n",
    "    accscores = []\n",
    "    precisionscores = []\n",
    "    recallscores = []\n",
    "    f1scores = []\n",
    "    \n",
    "\n",
    "    #Fold number, needed for stoing to csv\n",
    "    nrfold = 1\n",
    "\n",
    "    #Cross validation\n",
    "    for train, test in skfold.split(train_data, train_labels):\n",
    "        time_callback = TimeHistory() \n",
    "        \n",
    "        foldmodel = clone_model(model)\n",
    "        \n",
    "        #evaluate the positive class (=ironic data)\n",
    "        precision = km.binary_precision(label=1)\n",
    "        recall = km.binary_recall(label=1) \n",
    "        f1 = km.f1_score(label=1)\n",
    "        \n",
    "        foldmodel.compile(loss = lossfunction,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy',precision, recall, f1])\n",
    "\n",
    "        #Train the model with validation data splited through cross validation\n",
    "        hist= foldmodel.fit([crossvaldata[train], crossvaldata2[train],  crossvaldata3[train], crossvaldata4[train]], \n",
    "                            crossvallabels[train], epochs=nrepochs, batch_size=nrbatch, \n",
    "                            validation_data=([crossvaldata[test],crossvaldata2[test], crossvaldata3[test], crossvaldata4[test]], \n",
    "                            crossvallabels[test]), shuffle=False, verbose = 0, callbacks=[time_callback])\n",
    "\n",
    "        #Append average of the validation scores for the current fold\n",
    "        accscores.append(np.mean(hist.history['val_acc'])*100)\n",
    "        precisionscores.append(np.mean(hist.history['val_precision'])*100)\n",
    "        recallscores.append(np.mean(hist.history['val_recall'])*100)\n",
    "        f1scores.append(np.mean(hist.history['val_f1_score'])*100)\n",
    "\n",
    "        print(\"\\n-----Fold \"+str(nrfold)+\"--------\")\n",
    "        \n",
    "        #Plot the metrics for every epoch for the current fold\n",
    "        from matplotlib import pyplot\n",
    "        %matplotlib inline\n",
    "        pyplot.figure(figsize=(15,10))\n",
    "        pyplot.plot(hist.history['acc']) \n",
    "        pyplot.plot(hist.history['f1_score'])\n",
    "        pyplot.plot(hist.history['val_acc']) \n",
    "        pyplot.plot(hist.history['val_f1_score'])\n",
    "        #pyplot.plot(hist.history['loss'])\n",
    "        pyplot.xticks(np.arange(1,nrepochs))\n",
    "        pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "        pyplot.show()\n",
    "        \n",
    "        #Print the average validation scores for the current fold\n",
    "        print(\"Average Accuracy: %.2f%%\" % ((np.mean(hist.history['val_acc']))*100))\n",
    "        print(\"Average Precision: %.2f%%\" % ((np.mean(hist.history['val_precision']))*100))\n",
    "        print(\"Average Recall: %.2f%%\" % ((np.mean(hist.history['val_recall']))*100))\n",
    "        print(\"Average F1: %.2f%%\" % ((np.mean(hist.history['val_f1_score']))*100))\n",
    "\n",
    "        #Store the keras model\n",
    "        foldmodel.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                       '_'+str(nrfold)+'_model.h5')\n",
    "\n",
    "        #Store the train and validation scores for every epoch for the current fold to csv + train time\n",
    "        f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+'_'+\n",
    "                str(nrfold)+'_train_val_scores.csv','w+')\n",
    "        f.write('Epoch; Loss; Accuracy; Precision; Recall; F1; Val_loss; Val_Accuracy; Val_Precision; VAl_Recall; Val_F1; Time \\n')\n",
    "\n",
    "        for i in range(len(hist.history['acc'])):\n",
    "            f.write(str(i+1)+';'+str(hist.history['loss'][i])+';'+str(hist.history['acc'][i])+';'+\n",
    "                    str(hist.history['precision'][i])+';'+str(hist.history['recall'][i])+';'+\n",
    "                    str(hist.history['f1_score'][i])+';'+str(hist.history['val_loss'][i])+';'+\n",
    "                    str(hist.history['val_acc'][i])+';'+str(hist.history['val_precision'][i])+';'+\n",
    "                    str(hist.history['val_recall'][i])+';'+str(hist.history['val_f1_score'][i])+';'+\n",
    "                    str(time_callback.times[i])+'\\n')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        nrfold+=1\n",
    "      \n",
    "    #Plot and print the results of all folds \n",
    "    print(\"\\n-------Overallresults-------\")\n",
    "\n",
    "    #Plot the overall Metrics\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(accscores) \n",
    "    pyplot.plot(precisionscores)\n",
    "    pyplot.plot(recallscores)\n",
    "    pyplot.plot(f1scores)\n",
    "    #pyplot.xticks(np.arange(1,nrfold))\n",
    "    pyplot.legend(['Accuracy', 'Precision', 'Recall', 'F1'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Print mean, min and max values\n",
    "    print(\"Accuracy: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(accscores), np.std(accscores), np.max(accscores), np.min(accscores)))\n",
    "    print(\"Precision: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(precisionscores), np.std(precisionscores), np.max(precisionscores), np.min(precisionscores)))\n",
    "    print(\"Recall: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(recallscores), np.std(recallscores), np.max(recallscores), np.min(recallscores)))\n",
    "    print(\"F1: Mean = %.2f%% (+/- %.2f%%), Max = %.2f%%, Min =  %.2f%%\" % (np.mean(f1scores), np.std(f1scores), np.max(f1scores), np.min(f1scores)))\n",
    "    \n",
    "    #Write validation scores for every fold to csv\n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+\n",
    "            '_validationscores.csv',\"w+\")\n",
    "    f.write('Accuracy; Precision; Recall; F1 \\n')\n",
    "    for i in range(len(accscores)):\n",
    "        f.write(str(accscores[i])+';'+str(precisionscores[i])+';'+str(recallscores[i])+';'+str(f1scores[i])+'\\n')\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def train_evaluate_final_model(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_labels, test_data, test_labels, softmax, outputpath, outputname):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "        labelstest = to_categorical(test_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "        labelstest = test_labels\n",
    "\n",
    "    time_callback = TimeHistory() \n",
    "    \n",
    "    #evaluate the positive class (=ironic data)\n",
    "    precision =km.binary_precision(label=1)\n",
    "    recall = km.binary_recall(label=1)\n",
    "    f1 = km.f1_score(label=1)\n",
    "\n",
    "    model.compile(loss = lossfunction,\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy', precision, recall, f1])\n",
    "\n",
    "    #Train the model \n",
    "    hist= model.fit(train_data, labelstrain, epochs=nrepochs, batch_size=nrbatch, shuffle=True, verbose = 0, callbacks=[time_callback])\n",
    "\n",
    "    from matplotlib import pyplot\n",
    "    %matplotlib inline\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(hist.history['acc']) \n",
    "    pyplot.plot(hist.history['f1_score'])\n",
    "    #pyplot.plot(hist.history['loss'])\n",
    "    pyplot.xticks(np.arange(1,nrepochs))\n",
    "    pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Store the keras model\n",
    "    model.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                   '_'+'_model.h5')        \n",
    "    \n",
    "    score = model.evaluate(test_data, labelstest, verbose = 0)\n",
    "    \n",
    "    print(\"\\n-----Test Scores----\")\n",
    "    print(\"Loss: \" + str(score[0]))\n",
    "    print(\"Accuracy: \" + str(score[1]*100))\n",
    "    print(\"Precision: \" + str(score[2]*100))\n",
    "    print(\"Recall: \" + str(score[3]*100))\n",
    "    print(\"F1: \" + str(score[4]*100))\n",
    "\n",
    "\n",
    "    #stores the test results        \n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+'_finaltestresults.csv','w+')\n",
    "    f.write('Loss;'+str(score[0])+'\\n'+\n",
    "            'Accuracy;'+str(score[1])+'\\n'+\n",
    "            'Precision;'+str(score[2])+'\\n'+\n",
    "            'Recall;'+str(score[3])+'\\n'+\n",
    "            'F1;'+str(score[4])+'\\n')\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def train_evaluate_final_model_with_es(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_labels, test_data, test_labels, softmax, outputpath, outputname, es_monitor, es_patience):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "\n",
    "    time_callback = TimeHistory() \n",
    "    \n",
    "    #evaluate the positive class (=ironic data)\n",
    "    precision =km.binary_precision(label=1)\n",
    "    recall = km.binary_recall(label=1)\n",
    "    f1 = km.f1_score(label=1)\n",
    "\n",
    "    model.compile(loss = lossfunction,\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy', precision, recall, f1])\n",
    "    \n",
    "    es = EarlyStopping(monitor=es_monitor,\n",
    "                              min_delta=0,\n",
    "                              patience=es_patience,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "    #Train the model \n",
    "    hist= model.fit(train_data, labelstrain, validation_split = 0.2, epochs=nrepochs, batch_size=nrbatch, shuffle=True, verbose = 1, callbacks=[es, time_callback])\n",
    "\n",
    "    from matplotlib import pyplot\n",
    "    %matplotlib inline\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(hist.history['acc']) \n",
    "    pyplot.plot(hist.history['f1_score'])\n",
    "    #pyplot.plot(hist.history['loss'])\n",
    "    pyplot.xticks(np.arange(1,nrepochs))\n",
    "    pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Store the keras model\n",
    "    model.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                   '_'+'_model.h5')\n",
    "    \n",
    "    if softmax:\n",
    "        labelstest = to_categorical(test_labels)\n",
    "    else:\n",
    "        labelstest = test_labels\n",
    "    \n",
    "    score = model.evaluate(test_data, labelstest, verbose = 0)\n",
    "    \n",
    "    print(\"\\n-----Test Scores----\")\n",
    "    print(\"Loss: \" + str(score[0]))\n",
    "    print(\"Accuracy: \" + str(score[1]*100))\n",
    "    print(\"Precision: \" + str(score[2]*100))\n",
    "    print(\"Recall: \" + str(score[3]*100))\n",
    "    print(\"F1: \" + str(score[4]*100))\n",
    "\n",
    "\n",
    "    #stores the test results        \n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+'_finaltestresults.csv','w+')\n",
    "    f.write('Loss;'+str(score[0])+'\\n'+\n",
    "            'Accuracy;'+str(score[1])+'\\n'+\n",
    "            'Precision;'+str(score[2])+'\\n'+\n",
    "            'Recall;'+str(score[3])+'\\n'+\n",
    "            'F1;'+str(score[4])+'\\n')\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TerminateOnNaN\n",
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def train_evaluate_three_inputs_with_terminate(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_data2, train_data3,\n",
    "                                train_labels, test_data, test_data2, test_data3, test_labels, softmax, outputpath, outputname):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "        labelstest = to_categorical(test_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "        labelstest = test_labels\n",
    "\n",
    "    time_callback = TimeHistory() \n",
    "    terminate = TerminateOnNaN()\n",
    "    \n",
    "    #evaluate the positive class (=ironic data)\n",
    "    precision =km.binary_precision(label=1)\n",
    "    recall = km.binary_recall(label=1)\n",
    "    f1 = km.f1_score(label=1)\n",
    "\n",
    "    model.compile(loss = lossfunction,\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy', precision, recall, f1])\n",
    "\n",
    "    #Train the model \n",
    "    hist= model.fit([train_data, train_data2, train_data3], labelstrain, epochs=nrepochs, batch_size=nrbatch, \n",
    "                    shuffle=True, verbose = 0, callbacks=[time_callback, terminate])\n",
    "\n",
    "    from matplotlib import pyplot\n",
    "    %matplotlib inline\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(hist.history['acc']) \n",
    "    pyplot.plot(hist.history['f1_score'])\n",
    "    #pyplot.plot(hist.history['loss'])\n",
    "    pyplot.xticks(np.arange(1,nrepochs))\n",
    "    pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Store the keras model\n",
    "    model.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                   '_'+'_model.h5')        \n",
    "    \n",
    "    score = model.evaluate([test_data,test_data2, test_data3], labelstest, verbose = 0)\n",
    "    \n",
    "    print(\"\\n-----Test Scores----\")\n",
    "    print(\"Loss: \" + str(score[0]))\n",
    "    print(\"Accuracy: \" + str(score[1]*100))\n",
    "    print(\"Precision: \" + str(score[2]*100))\n",
    "    print(\"Recall: \" + str(score[3]*100))\n",
    "    print(\"F1: \" + str(score[4]*100))\n",
    "\n",
    "\n",
    "    #stores the test results        \n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+'_finaltestresults.csv','w+')\n",
    "    f.write('Loss;'+str(score[0])+'\\n'+\n",
    "            'Accuracy;'+str(score[1])+'\\n'+\n",
    "            'Precision;'+str(score[2])+'\\n'+\n",
    "            'Recall;'+str(score[3])+'\\n'+\n",
    "            'F1;'+str(score[4])+'\\n')\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def train_evaluate_two_inputs(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_data2, train_labels, \n",
    "                              test_data, test_data2, test_labels, softmax, outputpath, outputname):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "        labelstest = to_categorical(test_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "        labelstest = test_labels\n",
    "\n",
    "    time_callback = TimeHistory() \n",
    "    \n",
    "    #evaluate the positive class (=ironic data)\n",
    "    precision =km.binary_precision(label=1)\n",
    "    recall = km.binary_recall(label=1)\n",
    "    f1 = km.f1_score(label=1)\n",
    "\n",
    "    model.compile(loss = lossfunction,\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy', precision, recall, f1])\n",
    "\n",
    "    #Train the model \n",
    "    hist= model.fit([train_data, train_data2], labelstrain, epochs=nrepochs, batch_size=nrbatch, shuffle=True, verbose = 0, \n",
    "                    callbacks=[time_callback])\n",
    "\n",
    "    from matplotlib import pyplot\n",
    "    %matplotlib inline\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(hist.history['acc']) \n",
    "    pyplot.plot(hist.history['f1_score'])\n",
    "    #pyplot.plot(hist.history['loss'])\n",
    "    pyplot.xticks(np.arange(1,nrepochs))\n",
    "    pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Store the keras model\n",
    "    model.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                   '_'+'_model.h5')        \n",
    "    \n",
    "    score = model.evaluate([test_data,test_data2], labelstest, verbose = 0)\n",
    "    \n",
    "    print(\"\\n-----Test Scores----\")\n",
    "    print(\"Loss: \" + str(score[0]))\n",
    "    print(\"Accuracy: \" + str(score[1]*100))\n",
    "    print(\"Precision: \" + str(score[2]*100))\n",
    "    print(\"Recall: \" + str(score[3]*100))\n",
    "    print(\"F1: \" + str(score[4]*100))\n",
    "\n",
    "\n",
    "    #stores the test results        \n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+'_finaltestresults.csv','w+')\n",
    "    f.write('Loss;'+str(score[0])+'\\n'+\n",
    "            'Accuracy;'+str(score[1])+'\\n'+\n",
    "            'Precision;'+str(score[2])+'\\n'+\n",
    "            'Recall;'+str(score[3])+'\\n'+\n",
    "            'F1;'+str(score[4])+'\\n')\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def train_evaluate_three_inputs(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_data2, train_data3,\n",
    "                                train_labels, test_data, test_data2, test_data3, test_labels, softmax, outputpath, outputname):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "        labelstest = to_categorical(test_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "        labelstest = test_labels\n",
    "\n",
    "    time_callback = TimeHistory() \n",
    "    \n",
    "    #evaluate the positive class (=ironic data)\n",
    "    precision =km.binary_precision(label=1)\n",
    "    recall = km.binary_recall(label=1)\n",
    "    f1 = km.f1_score(label=1)\n",
    "\n",
    "    model.compile(loss = lossfunction,\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy', precision, recall, f1])\n",
    "\n",
    "    #Train the model \n",
    "    hist= model.fit([train_data, train_data2, train_data3], labelstrain, epochs=nrepochs, batch_size=nrbatch, \n",
    "                    shuffle=True, verbose = 0, callbacks=[time_callback])\n",
    "\n",
    "    from matplotlib import pyplot\n",
    "    %matplotlib inline\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(hist.history['acc']) \n",
    "    pyplot.plot(hist.history['f1_score'])\n",
    "    #pyplot.plot(hist.history['loss'])\n",
    "    pyplot.xticks(np.arange(1,nrepochs))\n",
    "    pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Store the keras model\n",
    "    model.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                   '_'+'_model.h5')        \n",
    "    \n",
    "    score = model.evaluate([test_data,test_data2, test_data3], labelstest, verbose = 0)\n",
    "    \n",
    "    print(\"\\n-----Test Scores----\")\n",
    "    print(\"Loss: \" + str(score[0]))\n",
    "    print(\"Accuracy: \" + str(score[1]*100))\n",
    "    print(\"Precision: \" + str(score[2]*100))\n",
    "    print(\"Recall: \" + str(score[3]*100))\n",
    "    print(\"F1: \" + str(score[4]*100))\n",
    "\n",
    "\n",
    "    #stores the test results        \n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+'_finaltestresults.csv','w+')\n",
    "    f.write('Loss;'+str(score[0])+'\\n'+\n",
    "            'Accuracy;'+str(score[1])+'\\n'+\n",
    "            'Precision;'+str(score[2])+'\\n'+\n",
    "            'Recall;'+str(score[3])+'\\n'+\n",
    "            'F1;'+str(score[4])+'\\n')\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Kearas model\n",
    "#softmax = boolean, true if the model uses softmaxe in the output layer otherwise false\n",
    "#outputpath = path were the output csv should be stored\n",
    "#outputname = name to use for the output csv\n",
    "def train_evaluate_four_inputs(model, nrepochs, nrbatch, optimizer, lossfunction, train_data, train_data2, train_data3, train_data4,\n",
    "                                train_labels, test_data, test_data2, test_data3, test_data4, test_labels, softmax, outputpath, outputname):\n",
    "    import datetime\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    rd.seed(seed)\n",
    "    \n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstrain = to_categorical(train_labels)\n",
    "        labelstest = to_categorical(test_labels)\n",
    "    else:\n",
    "        labelstrain = train_labels\n",
    "        labelstest = test_labels\n",
    "\n",
    "    time_callback = TimeHistory() \n",
    "    \n",
    "    #evaluate the positive class (=ironic data)\n",
    "    precision =km.binary_precision(label=1)\n",
    "    recall = km.binary_recall(label=1)\n",
    "    f1 = km.f1_score(label=1)\n",
    "\n",
    "    model.compile(loss = lossfunction,\n",
    "          optimizer=optimizer,\n",
    "          metrics=['accuracy', precision, recall, f1])\n",
    "\n",
    "    #Train the model \n",
    "    hist= model.fit([train_data, train_data2, train_data3, train_data4], labelstrain, epochs=nrepochs, batch_size=nrbatch, \n",
    "                    shuffle=True, verbose = 0, callbacks=[time_callback])\n",
    "\n",
    "    from matplotlib import pyplot\n",
    "    %matplotlib inline\n",
    "    pyplot.figure(figsize=(15,10))\n",
    "    pyplot.plot(hist.history['acc']) \n",
    "    pyplot.plot(hist.history['f1_score'])\n",
    "    #pyplot.plot(hist.history['loss'])\n",
    "    pyplot.xticks(np.arange(1,nrepochs))\n",
    "    pyplot.legend(['Train Accuracy', 'Train F1', 'Validation Accuracy', 'Validation F1', 'Loss'], loc=(1.04,0.5))\n",
    "    pyplot.show()\n",
    "\n",
    "    #Store the keras model\n",
    "    model.save(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+'_'+outputname+\n",
    "                   '_'+'_model.h5')        \n",
    "    \n",
    "    score = model.evaluate([test_data,test_data2, test_data3, test_data4], labelstest, verbose = 0)\n",
    "    \n",
    "    print(\"\\n-----Test Scores----\")\n",
    "    print(\"Loss: \" + str(score[0]))\n",
    "    print(\"Accuracy: \" + str(score[1]*100))\n",
    "    print(\"Precision: \" + str(score[2]*100))\n",
    "    print(\"Recall: \" + str(score[3]*100))\n",
    "    print(\"F1: \" + str(score[4]*100))\n",
    "\n",
    "\n",
    "    #stores the test results        \n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+'_finaltestresults.csv','w+')\n",
    "    f.write('Loss;'+str(score[0])+'\\n'+\n",
    "            'Accuracy;'+str(score[1])+'\\n'+\n",
    "            'Precision;'+str(score[2])+'\\n'+\n",
    "            'Recall;'+str(score[3])+'\\n'+\n",
    "            'F1;'+str(score[4])+'\\n')\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model with the held out train data\n",
    "def test_model(filename, testdata, test_labels, softmax, outputpath, outputname):\n",
    "    import datetime \n",
    "    finalmodel = load_model(str(filename), custom_objects={'binary_precision':km.binary_precision(label=1), 'binary_recall': km.binary_recall(label=1), 'binary_f1_score':km.f1_score(label=1), 'Attention':Attention()})\n",
    "\n",
    "    #needed for softmax-activation\n",
    "    if softmax:\n",
    "        labelstest = to_categorical(test_labels)\n",
    "    else:\n",
    "        labelstest = test_labels\n",
    "        \n",
    "    score = finalmodel.evaluate(testdata, labelstest, verbose = 0)\n",
    "    \n",
    "    print(\"\\n-----Test Scores----\")\n",
    "    print(\"Loss: \" + str(score[0]))\n",
    "    print(\"Accuracy: \" + str(score[1]*100))\n",
    "    print(\"Precision: \" + str(score[2]*100))\n",
    "    print(\"Recall: \" + str(score[3]*100))\n",
    "    print(\"F1: \" + str(score[4]*100))\n",
    "\n",
    "\n",
    "    #stores the test results        \n",
    "    f= open(outputpath+str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))+' '+outputname+'_finaltestresults.csv','w+')\n",
    "    f.write('Loss;'+str(score[0])+'\\n'+\n",
    "            'Accuracy;'+str(score[1])+'\\n'+\n",
    "            'Precision;'+str(score[2])+'\\n'+\n",
    "            'Recall;'+str(score[3])+'\\n'+\n",
    "            'F1;'+str(score[4])+'\\n')\n",
    "    f.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
