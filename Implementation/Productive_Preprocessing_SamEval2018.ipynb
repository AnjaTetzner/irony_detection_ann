{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anja Tetzner\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import regex as re\n",
    "import glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproducability of the results\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "rd.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "output= 'output_trainTestData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SAMEVAL = path + 'SemEval2018-T3-train-taskA_emoji.txt'\n",
    "GOLD_TEST_SAMEVAL  = path + 'SemEval2018-T3_gold_test_taskA_emoji.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train data, test data and word_index\n",
    "def create_train_test_wordindex(vocab, tweets, corpustrain):\n",
    "    MAX_NB_WORDS = len(vocab)\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH= 0\n",
    "    for tweet in tweets:\n",
    "        if len(tweet) > MAX_SEQUENCE_LENGTH:\n",
    "            MAX_SEQUENCE_LENGTH = len(tweet)\n",
    "            \n",
    "    print('Maximal Sequence Length: '+str(MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n",
    "\n",
    "    sequences = [[word_index.get(t, 0) for t in tweet]\n",
    "                 for tweet in tweets[:len(corpustrain)]]\n",
    "\n",
    "    test_sequences = [[word_index.get(t, 0) for t in tweet] \n",
    "                      for tweet in tweets[len(corpustrain):]]\n",
    "\n",
    "    train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                         padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "    test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n",
    "                              truncating=\"post\")\n",
    "\n",
    "    return train_data, test_data, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store data, labels and word_index for reproducability\n",
    "def write_file(filename, data):\n",
    "    file_present = glob.glob(filename)\n",
    "    if not file_present:\n",
    "        pd.DataFrame(data).to_csv(filename, header=None, index=None)\n",
    "    else:\n",
    "        print('Warining: File '+str(filename)+' already exists.')\n",
    "        \n",
    "def write_index(filename, index):\n",
    "    file_present = glob.glob(filename)\n",
    "    if not file_present:\n",
    "        f = open(filename, 'w+', encoding=\"utf-8\")\n",
    "        isnew = True\n",
    "        for key, value in index.items():\n",
    "            if isnew:\n",
    "                f.write(str(key)+'\\t'+str(value))\n",
    "                isnew = False\n",
    "            else:\n",
    "                f.write('\\n'+str(key)+'\\t'+str(value))\n",
    "        f.close()\n",
    "    else:\n",
    "        print('Warining: File '+str(filename)+' already exists.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(index_file, data_file):\n",
    "   \n",
    "    loaded_word_index = {}\n",
    "    with open(index_file, 'r', encoding='utf-8') as word_index: #Decoding utf-8, else exception\n",
    "        for line in word_index:\n",
    "            line = line.rstrip().split(\"\\t\")\n",
    "            loaded_word_index[line[0]] = line[1]  \n",
    "            \n",
    "    loaded_data =[]\n",
    "    with open(data_file, 'r', encoding='utf-8') as data: #Decoding utf-8, else exception\n",
    "        for line in data:\n",
    "            line = line.rstrip().split('\\n')\n",
    "            loaded_data.append(line) \n",
    "            \n",
    "    recunstructed_data = []\n",
    "    for line in loaded_data:\n",
    "        for string in line:\n",
    "            tweet = []\n",
    "            string = string.rstrip().split(',')\n",
    "            for item in string:\n",
    "                if item is not ('0'):\n",
    "                    for key, value in loaded_word_index.items():\n",
    "                        if value == item:\n",
    "                            tweet.append(key)\n",
    "            recunstructed_data.append(tweet)\n",
    "        \n",
    "    return recunstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_sequence(index_file, data_file, MAXLEN):\n",
    "   \n",
    "    recunstructed_data = reconstruct(index_file, data_file)\n",
    "            \n",
    "    pos = []\n",
    "    pos_list = []\n",
    "    for entry in recunstructed_data:\n",
    "        postags = nltk.pos_tag(entry)\n",
    "        for tag in postags:\n",
    "            if tag[1] not in pos_list:\n",
    "                pos_list.append(tag[1])\n",
    "        pos.append(postags)\n",
    "        \n",
    "    pos_dict ={}\n",
    "    i = 1\n",
    "\n",
    "    for entry in pos_list:\n",
    "        pos_dict[entry] = i\n",
    "        i += 1\n",
    "        \n",
    "    decoded_pos =[]\n",
    "    for entry in pos:\n",
    "        tweet = []\n",
    "        for tup in entry:\n",
    "            decode = pos_dict.get(tup[1])\n",
    "            tweet.append(decode)\n",
    "        decoded_pos.append(tweet)\n",
    "        \n",
    "    pos_sequences = pad_sequences(decoded_pos, maxlen=MAXLEN,padding=\"pre\", truncating=\"post\")\n",
    "    \n",
    "    return pos_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pos_file(filename, data):\n",
    "    file_present = glob.glob(filename)\n",
    "    if not file_present:\n",
    "        pd.DataFrame(data).to_csv(filename, header=None, index=None)\n",
    "    else:\n",
    "        print('Warining: File '+str(filename)+' already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_sentiment(data):\n",
    "    sentiment_blob = []\n",
    "\n",
    "    for entry in data:\n",
    "        sentiment_line = []\n",
    "        for word in entry:\n",
    "            word = TextBlob(word)\n",
    "            polarity = word.sentiment.polarity\n",
    "            sentiment_line.append(polarity)\n",
    "        sentiment_blob.append(sentiment_line)\n",
    "    return sentiment_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sentiment_file(filename, data, maxlength):\n",
    "    #data_sequence = pad_sequences(data, maxlen=MAXLEN,padding=\"pre\", truncating=\"post\")\n",
    "    file_present = glob.glob(filename)\n",
    "    pad_data= pad_sequences(data, maxlen=maxlength,padding=\"pre\", truncating=\"post\", dtype='float32')\n",
    "    if not file_present:\n",
    "        pd.DataFrame(pad_data).to_csv(filename, header=None, index=None)\n",
    "    else:\n",
    "        print('Warining: File '+str(filename)+' already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_samEval(FILENAME):\n",
    "    labels = []\n",
    "    corpus = []   \n",
    "    with open(FILENAME, 'r', encoding='utf-8') as train: #Decoding utf-8, else exception\n",
    "        for line in train:\n",
    "            if not line.lower().startswith(\"tweet index\"): #skip header\n",
    "                line = line.rstrip().split(\"\\t\")\n",
    "                label = line[1] #erste Spalte - label\n",
    "                labels.append(int(label))\n",
    "                tweet = line[2] #zweite Spalte - tweet\n",
    "                corpus.append(tweet)  \n",
    "    returnvalue = []\n",
    "    returnvalue.append(labels)\n",
    "    returnvalue.append(corpus)\n",
    "    return returnvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulttrain = read_data_samEval(TRAIN_SAMEVAL)\n",
    "labelstrain_sameval = resulttrain[0]\n",
    "corpustrain_sameval = resulttrain[1]\n",
    "\n",
    "resulttest = read_data_samEval(GOLD_TEST_SAMEVAL)\n",
    "labelstest_sameval = resulttest[0]\n",
    "corpustest_sameval = resulttest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train corpus: 3834\n",
      "Lenght train labels: 3834\n",
      "Lenght test corpus: 784\n",
      "lenght test labels: 784\n"
     ]
    }
   ],
   "source": [
    "print('Length train corpus: '+str(len(corpustrain_sameval)))\n",
    "print('Lenght train labels: '+str(len(labelstrain_sameval)))\n",
    "print('Lenght test corpus: '+str(len(corpustest_sameval)))\n",
    "print('lenght test labels: '+str(len(labelstest_sameval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sameval_lower_train_data = output + 'samEval_lower_train_data_preprocessed.csv'\n",
    "file_sameval_lower_train_labels = output + 'samEval_lower_train_labels_preprocessed.csv'\n",
    "file_sameval_lower_test_data = output + 'samEval_lower_test_data_preprocessed.csv'\n",
    "file_sameval_lower_test_labels = output + 'samEval_lower_test_labels_preprocessed.csv'\n",
    "file_sameval_lower_word_index = output + 'samEval_lower_word_index_preprocessed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess including lowercasing\n",
    "def text_to_wordlist_lower(tweet, vocab):\n",
    "    #Remove hashtags\n",
    "    tweet = re.sub('#','', tweet)\n",
    "    \n",
    "    #Remove usermentions\n",
    "    tweet= re.sub(r'(\\w+|^|)@\\w+','', tweet)\n",
    "    \n",
    "    #Treats url's as special tokens (actually twitter specific)\n",
    "    tweet=re.sub(r'((http|https)://)(\\w|[.]|/)+', 'URL', tweet)\n",
    "    \n",
    "    #Lowercasing\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    vocab.update(tweet)\n",
    "    return tweet\n",
    "\n",
    "def process_tweets_lower(list_sentences, vocab):\n",
    "    tweets = []\n",
    "    for tweet in list_sentences:\n",
    "        twt = text_to_wordlist_lower(tweet, vocab)\n",
    "        tweets.append(twt)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "vocab_lower = Counter()\n",
    "tweets_lower = process_tweets_lower(corpustrain_sameval + corpustest_sameval, vocab_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 40\n"
     ]
    }
   ],
   "source": [
    "train_data_lower, test_data_lower, word_index_lower = create_train_test_wordindex(vocab_lower, tweets_lower, corpustrain_sameval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(file_sameval_lower_train_data, train_data_lower)\n",
    "write_file(file_sameval_lower_train_labels, labelstrain_sameval)\n",
    "write_file(file_sameval_lower_test_data, test_data_lower)\n",
    "write_file(file_sameval_lower_test_labels, labelstest_sameval)\n",
    "write_index(file_sameval_lower_word_index, word_index_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If POS is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameval_lower_pos = create_pos_sequence(file_sameval_lower_word_index,file_sameval_lower_train_data, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pos_file(output+'samEval_lower_train_pos.csv', sameval_lower_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameval_lower_test_pos = create_pos_sequence(file_sameval_lower_word_index, file_sameval_lower_test_data, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pos_file(output+'samEval_lower_test_pos.csv', sameval_lower_test_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If Sentiment is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_data_lower = reconstruct(file_sameval_lower_word_index, file_sameval_lower_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_blob_lower = blob_sentiment(rec_data_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sentiment_file(output+'samEval_lower_train_data_sentiment_blob.csv', sentiment_blob_lower, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_data_test_lower = reconstruct(file_sameval_lower_word_index,file_sameval_lower_test_data)\n",
    "\n",
    "sentiment_blob_lower_test = blob_sentiment(rec_data_test_lower)\n",
    "\n",
    "write_sentiment_file(output+'samEval_lower_test_data_sentiment_blob.csv', sentiment_blob_lower_test,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sameval_stopword_train_data = output + 'sameval_stopword_train_data_preprocessed.csv'\n",
    "file_sameval_stopword_train_labels = output + 'sameval_stopword_train_labels_preprocessed.csv'\n",
    "file_sameval_stopword_test_data = output + 'sameval_stopword_test_data_preprocessed.csv'\n",
    "file_sameval_stopword_test_labels = output + 'sameval_stopword_test_labels_preprocessed.csv'\n",
    "file_sameval_stopword_word_index = output + 'sameval_stopword_word_index_preprocessed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess including stopwordremoval\n",
    "def text_to_wordlist_stopword(tweet, vocab):\n",
    "    #Remove hashtags\n",
    "    tweet = re.sub('#','', tweet)\n",
    "    \n",
    "    #Remove usermentions\n",
    "    tweet= re.sub(r'(\\w+|^|)@\\w+','', tweet)\n",
    "    \n",
    "    #Treats url's as special tokens (actually twitter specific)\n",
    "    tweet=re.sub(r'((http|https)://)(\\w|[.]|/)+', 'URL', tweet)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    #Remove stopwords\n",
    "    stopWords = stopwords.words('english')\n",
    "    wordsFiltered = []\n",
    "\n",
    "    for w in tweet:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    \n",
    "    vocab.update(wordsFiltered)\n",
    "    return wordsFiltered\n",
    "\n",
    "def process_tweets_stopword(list_sentences, vocab):\n",
    "    tweets = []\n",
    "    for tweet in list_sentences:\n",
    "        twt = text_to_wordlist_stopword(tweet, vocab)\n",
    "        tweets.append(twt)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "vocab_stopword = Counter()\n",
    "tweets_stopword = process_tweets_stopword(corpustrain_sameval + corpustest_sameval, vocab_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 33\n"
     ]
    }
   ],
   "source": [
    "train_data_stopword, test_data_stopword, word_index_stopword = create_train_test_wordindex(vocab_stopword, tweets_stopword, corpustrain_sameval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(file_sameval_stopword_train_data, train_data_stopword)\n",
    "write_file(file_sameval_stopword_train_labels, labelstrain_sameval)\n",
    "write_file(file_sameval_stopword_test_data, test_data_stopword)\n",
    "write_file(file_sameval_stopword_test_labels, labelstest_sameval)\n",
    "write_index(file_sameval_stopword_word_index, word_index_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If POS is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameval_stopword_pos = create_pos_sequence(file_sameval_stopword_word_index,file_sameval_stopword_train_data, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pos_file(output+'samEval_stopword_train_pos.csv', sameval_stopword_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameval_stopword_test_pos = create_pos_sequence(file_sameval_stopword_word_index, file_sameval_stopword_test_data, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pos_file(output+'samEval_stopword_test_pos.csv', sameval_stopword_test_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If Sentiment is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_data_stopword = reconstruct(file_sameval_stopword_word_index, file_sameval_stopword_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_blob_stopword = blob_sentiment(rec_data_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sentiment_file(output+'samEval_stopword_train_data_sentiment_blob.csv', sentiment_blob_stopword, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_data_test_stopword = reconstruct(file_sameval_stopword_word_index,file_sameval_stopword_test_data)\n",
    "\n",
    "sentiment_blob_stopword_test = blob_sentiment(rec_data_test_stopword)\n",
    "\n",
    "write_sentiment_file(output+'samEval_stopword_test_data_sentiment_blob.csv', sentiment_blob_stopword_test,33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sameval_punctuation_train_data = output + 'sameval_punctuation_train_data_preprocessed.csv'\n",
    "file_sameval_punctuation_train_labels = output + 'sameval_punctuation_train_labels_preprocessed.csv'\n",
    "file_sameval_punctuation_test_data = output + 'sameval_punctuation_test_data_preprocessed.csv'\n",
    "file_sameval_punctuation_test_labels = output + 'sameval_punctuation_test_labels_preprocessed.csv'\n",
    "file_sameval_punctuation_word_index = output + 'sameval_punctuation_word_index_preprocessed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess including punctuationremoval\n",
    "import string\n",
    "\n",
    "def text_to_wordlist_punctuation(tweet, vocab):\n",
    "    #Remove hashtags\n",
    "    tweet = re.sub('#','', tweet)\n",
    "    \n",
    "    #Remove usermentions\n",
    "    tweet= re.sub(r'(\\w+|^|)@\\w+','', tweet)\n",
    "    \n",
    "    #Treats url's as special tokens (actually twitter specific)\n",
    "    tweet=re.sub(r'((http|https)://)(\\w|[.]|/)+', 'URL', tweet)\n",
    "    \n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    vocab.update(tweet)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def process_tweets_punctuation(list_sentences, vocab):\n",
    "    tweets = []\n",
    "    for tweet in list_sentences:\n",
    "        twt = text_to_wordlist_punctuation(tweet, vocab)\n",
    "        tweets.append(twt)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "vocab_punctuation = Counter()\n",
    "tweets_punctuation = process_tweets_punctuation(corpustrain_sameval + corpustest_sameval, vocab_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 32\n"
     ]
    }
   ],
   "source": [
    "train_data_punctuation, test_data_punctuation, word_index_punctuation = create_train_test_wordindex(vocab_punctuation, tweets_punctuation, corpustrain_sameval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(file_sameval_punctuation_train_data, train_data_punctuation)\n",
    "write_file(file_sameval_punctuation_train_labels, labelstrain_sameval)\n",
    "write_file(file_sameval_punctuation_test_data, test_data_punctuation)\n",
    "write_file(file_sameval_punctuation_test_labels, labelstest_sameval)\n",
    "write_index(file_sameval_punctuation_word_index, word_index_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If POS is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameval_punctuation_pos = create_pos_sequence(file_sameval_punctuation_word_index,file_sameval_punctuation_train_data, 32)\n",
    "write_pos_file(output+'samEval_punctuation_train_pos.csv', sameval_punctuation_pos)\n",
    "\n",
    "sameval_punctuation_test_pos = create_pos_sequence(file_sameval_punctuation_word_index, file_sameval_punctuation_test_data, 32)\n",
    "write_pos_file(output+'samEval_punctuation_test_pos.csv', sameval_punctuation_test_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If Sentiment is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_data_punctuation = reconstruct(file_sameval_punctuation_word_index, file_sameval_punctuation_train_data)\n",
    "sentiment_blob_punctuation = blob_sentiment(rec_data_punctuation)\n",
    "write_sentiment_file(output+'samEval_punctuation_train_data_sentiment_blob.csv', sentiment_blob_punctuation, 32)\n",
    "\n",
    "rec_data_test_punctuation = reconstruct(file_sameval_punctuation_word_index,file_sameval_punctuation_test_data)\n",
    "sentiment_blob_punctuation_test = blob_sentiment(rec_data_test_punctuation)\n",
    "write_sentiment_file(output+'samEval_punctuation_test_data_sentiment_blob.csv', sentiment_blob_punctuation_test,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sameval_lemma_train_data = output + 'sameval_lemma_train_data_preprocessed.csv'\n",
    "file_sameval_lemma_train_labels = output + 'sameval_lemma_train_labels_preprocessed.csv'\n",
    "file_sameval_lemma_test_data = output + 'sameval_lemma_test_data_preprocessed.csv'\n",
    "file_sameval_lemma_test_labels = output + 'sameval_lemma_test_labels_preprocessed.csv'\n",
    "file_sameval_lemma_word_index = output + 'sameval_lemma_word_index_preprocessed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess including lemmaremoval\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def text_to_wordlist_lemma(tweet, vocab):\n",
    "    #Remove hashtags\n",
    "    tweet = re.sub('#','', tweet)\n",
    "    \n",
    "    #Remove usermentions\n",
    "    tweet= re.sub(r'(\\w+|^|)@\\w+','', tweet)\n",
    "    \n",
    "    #Treats url's as special tokens (actually twitter specific)\n",
    "    tweet=re.sub(r'((http|https)://)(\\w|[.]|/)+', 'URL', tweet)\n",
    "\n",
    "    #Tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    #Lemmatization\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    tweet_lemma =[]\n",
    "    for word in tweet:\n",
    "        tweet_lemma.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "    \n",
    "    return tweet_lemma\n",
    "\n",
    "def process_tweets_lemma(list_sentences, vocab):\n",
    "    tweets = []\n",
    "    for tweet in list_sentences:\n",
    "        twt = text_to_wordlist_lemma(tweet, vocab)\n",
    "        tweets.append(twt)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "vocab_lemma = Counter()\n",
    "tweets_lemma = process_tweets_lemma(corpustrain_sameval + corpustest_sameval, vocab_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 40\n"
     ]
    }
   ],
   "source": [
    "train_data_lemma, test_data_lemma, word_index_lemma = create_train_test_wordindex(vocab_lemma, tweets_lemma, corpustrain_sameval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(file_sameval_lemma_train_data, train_data_lemma)\n",
    "write_file(file_sameval_lemma_train_labels, labelstrain_sameval)\n",
    "write_file(file_sameval_lemma_test_data, test_data_lemma)\n",
    "write_file(file_sameval_lemma_test_labels, labelstest_sameval)\n",
    "write_index(file_sameval_lemma_word_index, word_index_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If POS is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameval_lemma_pos = create_pos_sequence(file_sameval_lemma_word_index,file_sameval_lemma_train_data, 40)\n",
    "write_pos_file(output+'samEval_lemma_train_pos.csv', sameval_lemma_pos)\n",
    "\n",
    "sameval_lemma_test_pos = create_pos_sequence(file_sameval_lemma_word_index, file_sameval_lemma_test_data, 40)\n",
    "write_pos_file(output+'samEval_lemma_test_pos.csv', sameval_lemma_test_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If Sentiment is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_data_lemma = reconstruct(file_sameval_lemma_word_index, file_sameval_lemma_train_data)\n",
    "sentiment_blob_lemma = blob_sentiment(rec_data_lemma)\n",
    "write_sentiment_file(output+'samEval_lemma_train_data_sentiment_blob.csv', sentiment_blob_lemma, 40)\n",
    "\n",
    "rec_data_test_lemma = reconstruct(file_sameval_lemma_word_index,file_sameval_lemma_test_data)\n",
    "sentiment_blob_lemma_test = blob_sentiment(rec_data_test_lemma)\n",
    "write_sentiment_file(output+'samEval_lemma_test_data_sentiment_blob.csv', sentiment_blob_lemma_test,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination best preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sameval_bestpp_train_data = output + 'sameval_bestpp_train_data_preprocessed.csv'\n",
    "file_sameval_bestpp_train_labels = output + 'sameval_bestpp_train_labels_preprocessed.csv'\n",
    "file_sameval_bestpp_test_data = output + 'sameval_bestpp_test_data_preprocessed.csv'\n",
    "file_sameval_bestpp_test_labels = output + 'sameval_bestpp_test_labels_preprocessed.csv'\n",
    "file_sameval_bestpp_word_index = output + 'sameval_bestpp_word_index_preprocessed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess including best methods\n",
    "def text_to_wordlist_bestpp(tweet, vocab):\n",
    "    #Remove hashtags\n",
    "    tweet = re.sub('#','', tweet)\n",
    "    \n",
    "    #Remove usermentions\n",
    "    tweet= re.sub(r'(\\w+|^|)@\\w+','', tweet)\n",
    "    \n",
    "    #Treats url's as special tokens (actually twitter specific)\n",
    "    tweet=re.sub(r'((http|https)://)(\\w|[.]|/)+', 'URL', tweet)\n",
    "    \n",
    "    #Lowercasing\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    #Remove stopwords\n",
    "    stopWords = stopwords.words('english')\n",
    "    wordsFiltered = []\n",
    "\n",
    "    for w in tweet:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    \n",
    "    vocab.update(wordsFiltered)\n",
    "    return wordsFiltered\n",
    "\n",
    "def process_tweets_bestpp(list_sentences, vocab):\n",
    "    tweets = []\n",
    "    for tweet in list_sentences:\n",
    "        twt = text_to_wordlist_bestpp(tweet, vocab)\n",
    "        tweets.append(twt)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "vocab_bestpp = Counter()\n",
    "tweets_bestpp = process_tweets_bestpp(corpustrain_sameval + corpustest_sameval, vocab_bestpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 33\n"
     ]
    }
   ],
   "source": [
    "train_data_bestpp, test_data_bestpp, word_index_bestpp = create_train_test_wordindex(vocab_bestpp, tweets_bestpp, corpustrain_sameval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(file_sameval_bestpp_train_data, train_data_bestpp)\n",
    "write_file(file_sameval_bestpp_train_labels, labelstrain_sameval)\n",
    "write_file(file_sameval_bestpp_test_data, test_data_bestpp)\n",
    "write_file(file_sameval_bestpp_test_labels, labelstest_sameval)\n",
    "write_index(file_sameval_bestpp_word_index, word_index_bestpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If POS is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameval_bestpp_pos = create_pos_sequence(file_sameval_bestpp_word_index,file_sameval_bestpp_train_data, 33)\n",
    "write_pos_file(output+'samEval_bestpp_train_pos.csv', sameval_bestpp_pos)\n",
    "\n",
    "sameval_bestpp_test_pos = create_pos_sequence(file_sameval_bestpp_word_index, file_sameval_bestpp_test_data, 33)\n",
    "write_pos_file(output+'samEval_bestpp_test_pos.csv', sameval_bestpp_test_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If Sentiment is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_data_bestpp = reconstruct(file_sameval_bestpp_word_index, file_sameval_bestpp_train_data)\n",
    "sentiment_blob_bestpp = blob_sentiment(rec_data_bestpp)\n",
    "write_sentiment_file(output+'samEval_bestpp_train_data_sentiment_blob.csv', sentiment_blob_bestpp, 33)\n",
    "\n",
    "rec_data_test_bestpp = reconstruct(file_sameval_bestpp_word_index,file_sameval_bestpp_test_data)\n",
    "sentiment_blob_bestpp_test = blob_sentiment(rec_data_test_bestpp)\n",
    "write_sentiment_file(output+'samEval_bestpp_test_data_sentiment_blob.csv', sentiment_blob_bestpp_test,33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sameval_username_train_data = output + 'sameval_username_train_data_preprocessed.csv'\n",
    "file_sameval_username_train_labels = output + 'sameval_username_train_labels_preprocessed.csv'\n",
    "file_sameval_username_test_data = output + 'sameval_username_test_data_preprocessed.csv'\n",
    "file_sameval_username_test_labels = output + 'sameval_username_test_labels_preprocessed.csv'\n",
    "file_sameval_username_word_index = output + 'sameval_username_word_index_preprocessed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess including best methods\n",
    "def text_to_wordlist_username(tweet, vocab):\n",
    "    #Remove hashtags\n",
    "    tweet = re.sub('#','', tweet)\n",
    "    \n",
    "    #Treats usermentions as special tokens\n",
    "    tweet= re.sub(r'(\\w+|^|)@\\w+','USERNAME', tweet)\n",
    "    \n",
    "    #Treats url's as special tokens (actually twitter specific)\n",
    "    tweet=re.sub(r'((http|https)://)(\\w|[.]|/)+', 'URL', tweet)\n",
    "    \n",
    "    #Lowercasing\n",
    "    #tweet = tweet.lower()\n",
    "    \n",
    "    #Tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    #Remove stopwords\n",
    "    #stopWords = stopwords.words('english')\n",
    "    #wordsFiltered = []\n",
    "\n",
    "    #for w in tweet:\n",
    "      #  if w not in stopWords:\n",
    "       #     wordsFiltered.append(w)\n",
    "    \n",
    "    vocab.update(tweet)\n",
    "    return tweet\n",
    "\n",
    "def process_tweets_username(list_sentences, vocab):\n",
    "    tweets = []\n",
    "    for tweet in list_sentences:\n",
    "        twt = text_to_wordlist_username(tweet, vocab)\n",
    "        tweets.append(twt)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocess the data\n",
    "vocab_username = Counter()\n",
    "tweets_username = process_tweets_username(corpustrain_sameval + corpustest_sameval, vocab_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal Sequence Length: 40\n"
     ]
    }
   ],
   "source": [
    "train_data_username, test_data_username, word_index_username = create_train_test_wordindex(vocab_username, tweets_username, corpustrain_sameval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(file_sameval_username_train_data, train_data_username)\n",
    "write_file(file_sameval_username_train_labels, labelstrain_sameval)\n",
    "write_file(file_sameval_username_test_data, test_data_username)\n",
    "write_file(file_sameval_username_test_labels, labelstest_sameval)\n",
    "write_index(file_sameval_username_word_index, word_index_username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If POS is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameval_username_pos = create_pos_sequence(file_sameval_username_word_index,file_sameval_username_train_data, 40)\n",
    "write_pos_file(output+'samEval_username_train_pos.csv', sameval_username_pos)\n",
    "\n",
    "sameval_username_test_pos = create_pos_sequence(file_sameval_username_word_index, file_sameval_username_test_data, 40)\n",
    "write_pos_file(output+'samEval_username_test_pos.csv', sameval_username_test_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If Sentiment is created directly after creating the train and test data, then tweets_lower could be used instad of recreating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_data_username = reconstruct(file_sameval_username_word_index, file_sameval_username_train_data)\n",
    "sentiment_blob_username = blob_sentiment(rec_data_username)\n",
    "write_sentiment_file(output+'samEval_username_train_data_sentiment_blob.csv', sentiment_blob_username, 40)\n",
    "\n",
    "rec_data_test_username = reconstruct(file_sameval_username_word_index,file_sameval_username_test_data)\n",
    "sentiment_blob_username_test = blob_sentiment(rec_data_test_username)\n",
    "write_sentiment_file(output+'samEval_username_test_data_sentiment_blob.csv', sentiment_blob_username_test,40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
